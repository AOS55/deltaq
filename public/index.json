[{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. A robot placing balls in a pot.\nYour browser does not support the video tag. A baby placing balls in a pot.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nBasic familiarity with machine learning concepts (neural networks, gradient descent). Some understanding of probability and linear algebra. Basic programming knowledge. No robotics or control theory background needed - we\u0026rsquo;ll build these concepts from the ground up. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eIn 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like \u003ca href=\"https://www.youtube.com/watch?v=rDxTsjD-dKw\"\u003emixing a drink\u003c/a\u003e, \u003ca href=\"https://www.youtube.com/watch?v=E2evC2xTNWg\"\u003emaking breakfast\u003c/a\u003e or \u003ca href=\"https://www.youtube.com/watch?v=g0TaYhjpOfo\"\u003ewalking on uneven ground\u003c/a\u003e, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like \u003ca href=\"https://www.chess.com/terms/alphazero-chess-engine\"\u003eplaying chess\u003c/a\u003e or \u003ca href=\"https://www.nature.com/articles/d41586-025-00406-7\"\u003eproving theorems\u003c/a\u003e, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\u003c/p\u003e\n\u003cp\u003eThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nBasic familiarity with machine learning concepts (neural networks, gradient descent). Some understanding of probability and linear algebra. Basic programming knowledge. No robotics or control theory background needed - we\u0026rsquo;ll build these concepts from the ground up. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nBasic familiarity with machine learning concepts (neural networks, gradient descent). Some understanding of probability and linear algebra. Basic programming knowledge. No robotics or control theory background needed - we\u0026rsquo;ll build these concepts from the ground up. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, A. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nBasic familiarity with machine learning concepts (neural networks, gradient descent). Some understanding of probability and linear algebra. Basic programming knowledge. No robotics or control theory background needed - we\u0026rsquo;ll build these concepts from the ground up. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nBasic familiarity with machine learning concepts (neural networks, gradient descent). Some understanding of probability and linear algebra. Basic programming knowledge. No robotics or control theory background needed - we\u0026rsquo;ll build these concepts from the ground up. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nBasic familiarity with machine learning concepts (neural networks, gradient descent). Some understanding of probability and linear algebra. Basic programming knowledge. No robotics or control theory background needed - we\u0026rsquo;ll build these concepts from the ground up. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming knowledge. No robotics or control theory background needed - we\u0026rsquo;ll build these concepts from the ground up. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. No robotics or control theory background needed - we\u0026rsquo;ll build these concepts from the ground up. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nMy started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMy started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nI started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is a key design point.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nI started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some open-source scripts for a robotic arm to solve a pick and place task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of robotic learning this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described - is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} ​\\max_{​D} \\mathbb{E}_{\\pi}​[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}​[\\log(1−D(s_{t},a_{t}))]−\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal - perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}∣s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}​=\\arg\\max_{a_{t:t+H}}​ \\sum_{h=0}^{H} ​r(s_{h}​,a_{h}​) \\ \\text{where} \\ s_{h+1}​=f_{\\theta}​(s_{h}​,a_{h}​) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong, Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo; Progress in Brain Research, vol. 192, pp. 81-97, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, “EPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 4115–4131, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, “ALVINN: An Autonomous Land Vehicle in a Neural Network,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989, pp. 305–313.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, “Generative Adversarial Imitation Learning,” in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016, pp. 4565–4573.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,” in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011, pp. 627–635.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, “ThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,” in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020, pp. 1165–1172.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Zhang and D. Held, “SafeDAgger: Safely Interacting with Human Teachers in Deep Learning for Robotics,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 614–621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, “Reinforcement and Imitation Learning via Interactive No-Regret Learning,” arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., “Human-level control through deep reinforcement learning,” in Nature, vol. 518, no. 7540, pp. 529–533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-Dimensional Continuous Control Using Generalized Advantage Estimation,” in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust Region Policy Optimization,” in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,” in International Conference on Machine Learning (ICML), 2018, pp. 1861–1870.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, “Double Q-learning,” in Advances in Neural Information Processing Systems (NeurIPS), 2010, pp. 2613–2621.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, “Auto-Encoding Variational Bayes,” in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, “Demonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,” in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, “Model predictive path integral control: Information theoretic model predictive control,” in IEEE International Conference on Robotics and Automation (ICRA), 2017, pp. 3192–3199.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,” in Advances in Neural Information Processing Systems (NeurIPS), 2018, pp. 4759–4770.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. (1991). “Dyna, an Integrated Architecture for Learning, Planning, and Reacting.” SIGART Bulletin, 2(4), 160–163.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, “When to Trust Your Model: Model-Based Policy Optimization,” in Advances in Neural Information Processing Systems (NeurIPS), 2019, pp. 12498–12509.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-End Object Detection with Transformers,” arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, “DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,” arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,” in European Conference on Computer Vision (ECCV), 2018, pp. 833–851.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, “UNet++: A Nested U-Net Architecture for Medical Image Segmentation,” in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018, pp. 3–11.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, “Fast-SCNN: Fast Semantic Segmentation Network,” in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. Dollár, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, “FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, “The Unscented Kalman Filter for Nonlinear Estimation,” in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, “DeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,” in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019, pp. 6906-6913, doi: 10.1109/IROS40897.2019.8968467.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQin, P. Li, and S. Shen, “VINS-Mono: A robust and versatile monocular visual-inertial state estimator,” IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. Fácil, J. Civera, and J. Neira, “DynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,” IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 4076–4083, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, “Robust Map Optimization Using Dynamic Covariance Scaling,” in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, “Robust Visual SLAM Across Seasons,” in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, “Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,” IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309–1332, 2016\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post I have also written some \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eopen-source scripts\u003c/a\u003e for a robotic arm to solve a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick and place\u003c/a\u003e task, similar to our coffee cup examples, using each of the methods discussed. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e this programme also includes pre-trained models that can be downloaded from Hugging Faces. Please feel free to modify and use them as you see fit they principally show how to use the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}​,a_{0}​,s_{1}​,a_{1}​,...,s_{T}​) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233–242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called ∇Q ? A couple of reasons:\nI started out in aerospace and max-Q (∇Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called ∇Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (∇Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"}]