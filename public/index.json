[{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFinal Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field however is at a critical but exciting juncture where architectural innovation, rather than simply scaling model size, may prove more valuable for practical deployment.\nCitation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field however is at a critical but exciting juncture where architectural innovation, rather than simply scaling model size, may prove more valuable for practical deployment.\nCitation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field however is at a critical but exciting juncture where architectural innovation, rather than simply scaling model size, may prove more valuable for practical deployment.\nCitation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field however is at a critical but exciting juncture where architectural innovation, rather than simply scaling model size, may prove more valuable for practical deployment.\nCitation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field however is at a critical but exciting juncture where architectural innovation, rather than simply scaling model size, may prove more valuable for practical deployment.\nCitation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field however is at a critical but exciting juncture where architectural innovation, rather than simply scaling model size, may prove more valuable for practical deployment.\nCitation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field is however still in its infancy, having written this section and researching VLAs I think the following questions are some of the most critical to answer in the short to medium term:\nWhat matters in Sim2Real for VLAs, we understand that VLAs need finetuning to specific tasks but what specifically is disruptive for VLAs. Understanding this is key to understand how to deploy VLAs in real systems. How should VLAs be deployed, should we focus on designing models that on robots themeselves or using larger models running on data centers. How do we manage VLA hallucination, hallucination is well studied in LLMs and several methods have been developed to mitigate them. How do we recover from bad plans or hallucination in VLAs, can we do something similar to Recover? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field is however still in its infancy, having written this section and researching VLAs I think the following questions are some of the most critical to answer in the short to medium term:\nWhat matters in Sim2Real for VLAs, we understand that VLAs need finetuning to specific tasks but what specifically is disruptive for VLAs. Understanding this is key to understand how to deploy VLAs in real systems. How should VLAs be deployed, should we focus on designing models that on robots themselves or using larger models running on data centers. How do we manage VLA hallucination, hallucination is well studied in LLMs and several methods have been developed to mitigate them. How do we recover from bad plans or hallucination in VLAs, can we do something similar to this? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems and companies are beginning to heavily invest into developing larger and more capable models. The field is however still in its infancy, having written this section and researching VLAs I think the following questions are some of the most critical to answer in the short to medium term:\nWhat matters in Sim2Real for VLAs, we understand that VLAs need fine-tuning to specific tasks but what specifically is disruptive for VLAs. Understanding this is key to understand how to deploy VLAs in real systems. How should VLAs be deployed, should we focus on designing models that on robots themselves or using larger models running on data centers. How do we manage VLA hallucination, hallucination is well studied in LLMs and several methods have been developed to mitigate them. How do we recover from bad plans or hallucination in VLAs, can we do something similar to this? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that would be great to see more work in the short to medium-term trajectory of the field:\nWhat matters in Sim2Real for VLAs, we understand that VLAs need fine-tuning to specific tasks but what specifically is disruptive for VLAs. Understanding this is key to understand how to deploy VLAs in real systems. How should VLAs be deployed, should we focus on designing models that on robots themselves or using larger models running on data centers. How do we manage VLA hallucination, hallucination is well studied in LLMs and several methods have been developed to mitigate them. How do we recover from bad plans or hallucination in VLAs, can we do something similar to this? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs?. While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute?, Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we manage VLA hallucination, hallucination is well studied in LLMs and several methods have been developed to mitigate them. How do we recover from bad plans or hallucination in VLAs, can we do something similar to this? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs?. While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute?, Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on reinforcement learning for policy optimization, but VLAs introduce a new paradigm with their foundation model architecture. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on reinforcement learning for policy optimization, but VLAs introduce a new paradigm with their foundation model architecture. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on reinforcement learning for policy optimization, but VLAs introduce a new paradigm with their foundation model architecture. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on RL for policy optimization. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; K Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on RL for policy optimization. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Wang, H Zhu, M Liu, J Yang, HS Fang, T He, VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers, arXiv:2507.01016, 2025\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations. However, these successes come with important caveats: deployed systems operate in carefully constrained environments with extensive safety monitoring, and performance remains sensitive to environmental variations that would be routine for human workers.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on RL for policy optimization. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Wang, H Zhu, M Liu, J Yang, HS Fang, T He, VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers, arXiv:2507.01016, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks42. VLA models remain susceptible to patch-based attacks43 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$44 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$44 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on RL for policy optimization. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Wang, H Zhu, M Liu, J Yang, HS Fang, T He, VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers, arXiv:2507.01016, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on RL for policy optimization. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Wang, H Zhu, M Liu, J Yang, HS Fang, T He, VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers, arXiv:2507.01016, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on RL for policy optimization. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Wang, H Zhu, M Liu, J Yang, HS Fang, T He, VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers, arXiv:2507.01016, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately, Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on RL for policy optimization. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Wang, H Zhu, M Liu, J Yang, HS Fang, T He, VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers, arXiv:2507.01016, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"},{"content":"If I could use one word to describe the advancement of ML or AI in the past couple of years it would be scale. When GPT-31 was released in 2020 and ChatGPT2 in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how AlexNet3 allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine recently described this as:\nA behavioral model of what humans tend to do with a computer, keyboard, and mouse.\nFollowing the explosion of LLMs, labs with the resources to do so, have begun to develop large scale models for robotics. These scaled-up robotic models have become known as foundation models, serving as the base upon which entire robotic platforms are built. I prefer this term over large since what constitutes large today often becomes small tomorrow. Figure 1 shows the number of parameters each model has against time, though I couldn\u0026rsquo;t find a suitable benchmark for comparison, an issue I will come to later on. Unlike in the LLM space, there isn\u0026rsquo;t a clear bigger is better trend emerging in robotics. Whilst I suspect the recently released Gemini Robotics VLA4 could be very large given the trend from RT-2 the model specifics are not included in the paper. The bigger is better trend hasn\u0026rsquo;t proven true for robotic foundation models, at least not yet. In this article I aim to explore:\nHow foundation models work: The architectural principles behind robotic foundation models, including how they integrate multi-modal data (vision, language, motor control) and differ from pure language models in their design and training approaches. Applications and data collection: Real-world use cases where these models are being deployed, the types of robotics data being collected to train them, and how this data differs from the internet-scale text that powers LLMs. Current problems and emerging solutions: The key limitations facing robotic foundation models today (scaling challenges, benchmarking gaps, deployment issues) and the research directions and technical approaches being developed to address them. Foundation Models To understand how foundation models work, let\u0026rsquo;s first briefly examine how LLMs work, as these form the basis of how foundation models are applied across different domains. Modern LLM development follows a two-stage process: pre-training and post-training. Pre-training involves training the core LLM architecture on large datasets to learn language patterns and world knowledge. Post-training5 then fine-tunes the model through techniques like Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to improve alignment and task-specific capabilities.\nThe general objective function of an LLM during pre-training can be thought of as:\nGiven a sequence of words, predict the most likely next word.\nThis process involves 3 key components/processes:\nEmbedding, converts text into a tokenized vector representation. Tokenization first breaks text into subword units (tokens), which are then mapped to learned vector embeddings which capture the semantic meaning in high-dimensional space. Transformers, are the main building blocks of the LLM architecture. Each transformer contains an attention mechanism that allows tokens to selectively focus on and communicate with other relevant tokens in the sequence. Typically, a Multi Layer Perceptron (MLP) further refines each token\u0026rsquo;s representation. Multiple transformer layers are stacked on top of each other to build deep networks. Output Probabilities, the final linear and softmax layers transform the processed embeddings into a probability distribution over the entire vocabulary, determining which token is most likely to come next. Several excellent resources help explain how transformers and LLMs work. I particularly recommend this visualisation. Figure 2 shows the general structure of LLMs as a block architecture.\nFigure 2: LLM Block architecture. For language models and foundation models in general, it is best to think of everything as vectors. This vector representation allows mathematical operations and enables models to process and manipulate semantic information numerically. The input can be represented as a vector:\n$$ \\mathbf{x} = (x_{0}, x_{1}, x_{2}, \\ldots, x_{n}). $$The prevailing approach to large scale modelling are autoregressive where the output is represented as:\n$$ P(\\mathbf{y}|\\mathbf{x}) = \\prod_{i=1}^{n} P(y_{i} | \\mathbf{x}, y_{1:i-1}). $$This equation represents the chain rule of probability, where $y_{1:i-1}$ denotes all previous tokens up to position $i-1$. In practical terms, this autoregressive approach means that LLMs generate text one token at a time, with each new token conditioned on both the original input and all previously generated tokens.\nGeneral Foundation Models While LLMs exclusively process text tokens, the same transformer architecture and training methodology can be adapted to handle other types of sequential data including:\nImages, are divided into patches and treated as visual tokens. For example CLIP6 learns joint image-text representations through contrastive training on (image, text) pairs. Audio spectrograms, are tokenized as spectogram patches for audio classification7. Time series, data is segmented into windows for forecasting8 and anomaly detection9. Action sequences, can be tokenised for RL. Decision Transformer10 eliminates value functions entirely by framing RL as a conditional sequence modelling problem. Multimodal inputs, combine multiple token types. Flamingo11, is an early example of interleaving vision-language sequences. Most modern frontier labs offer multimodal versions of their large-language models. Modern multi-modal examples/foundation models include Meta\u0026rsquo;s Llama12, X.AIs Grok-1.5v, Anthropic\u0026rsquo;s Claude, OpenAI\u0026rsquo;s GPT4o13 and Google/DeepMind\u0026rsquo;s Gemini14. These can handle text, images, audio and video. Robotic Foundation Models Foundation models for robotics face a fundamentally different challenge than pure language models, the need to interact with the physical world. While LLMs predict the next token in a text sequence, robotic foundation models must predict actions that will be executed by physical systems in the real world. This shift from virtual to physical introduces several key differences. Robotic foundation models need to:\nUnderstand the embodiment constraints of the robot, meaning the model must comprehend the robots physical limitations, spatial relationships and potential outcomes. The model must also run in real-time creating lower latency demands for safe operation. Multimodal integration is essential as robots need to process vision, proprioception, language instructions and other sensor inputs simultaneously. The most successful robotic foundation models follow the Vision-Language-Action (VLA) paradigm, which extends the transformer architecture to handle three key modalities:\nVision, processes camera feeds and depth sensors tokenised as image patches. Language, handles natural language instructions and task descriptions. Action, converts robot joint commands and end-effector movements into discrete tokens. RT-115 (Robot Transformer) was the first robotic foundation model, developed by Google Robotics and Everyday Robotics in 2022. The system was trained on approximately 130,000 robot demonstrations collected over 17 months using a fleet of 13 robots, covering over 700 distinct tasks across multiple kitchen-based environments. RT-1 was trained and deployed on Everyday Robots mobile manipulator robots, a 7 degree of freedom robot with a 2 finger gripper and mobile base shown in Figure 3.\nFigure 3: Everyday Robot platform. RT-1\u0026rsquo;s architecture is designed for both high performance and real-time operation. The system takes natural language instructions and images from 6 cameras as input, processing them through a FiLM-conditioned EfficientNet-B316 that combines language and vision information early in the pipeline. The resulting 81 tokens are compressed to just 8 tokens using TokenLearner17, which retains only the most important information. These compressed tokens feed into a Transformer with 8 attention layers that outputs discrete action commands across 11 dimensions: 7 for arm movement, 3 for base movement, and 1 for mode selection, with each dimension divided into 256 possible values. Despite having 35 million parameters, this design runs at 3 Hz for real-time robot control.\nFigure 4: RT1 Architecture. Advancing VLAs Over the past several years LLM developers have leveraged massive datasets scraped from the internet to rapidly develop their model capabilities. Robotics doesn\u0026rsquo;t have this luxury. Unlike text, which exists abundantly online, robotic learning requires physical interaction data: demonstrations of robots manipulating objects, navigating environments, and performing tasks in the real world. This embodied data is expensive to collect, difficult to standardize across different robotic platforms, and challenging to scale. As a result, robotics lacks the massive, unified datasets that have powered breakthroughs in NLP, creating a significant bottleneck for developing capable robot foundation models.\nThis has pushed robotics labs to develop several novel approaches towards data collection and model design. Collaborative data collection across different laboratories and robot types is one promising direction, though the largest dataset currently available, the Open-X Embodiment Dataset18 is still relatively limited. Out of 73 datasets, 55 focus on single-arm manipulation with tabletop setups using toy kitchen objects, and data collection still predominantly relies on human experts using VR or haptic devices. Companies like Covariant have found success combining real-world data with synthetic data, while others are exploring reinforcement learning in production environments.\nEvaluating progress across these diverse approaches remains challenging since current VLA models show significant variation in performance across different tasks and robot platforms, and there\u0026rsquo;s no standardized robotic setup. However, several key metrics have emerged that are useful for practitioners and have generally shown improvement across VLA development:\nInference Speed measures how quickly the model can generate actions. Unlike LLMs where users can tolerate multi-second response times, robots require real-time control, modern VLAs achieve anywhere from 6Hz (OpenVLA) to 120Hz (GR00T N1\u0026rsquo;s fast system). Memory Requirements determine the hardware needed for deployment. VLAs face unique constraints compared to LLMs since they often must run on-device or locally to minimize response latency. Recent advances in quantization and architecture design have reduced model memory footprints significantly. Training Efficiency encompasses both initial training time and fine-tuning speed for new tasks or robot platforms. Since the deployment platform often differs from the training environment, efficient adaptation becomes crucial. Modern approaches like LoRA fine-tuning can reduce training time by 70%, while some models now require as few as 50 demonstration episodes to learn new behaviors. Beyond these quantifiable metrics, VLAs have improved in areas that are more challenging to measure directly, such as zero-shot generalization to novel objects, cross-task transfer capabilities, and overall success rates across diverse manipulation scenarios. These improvements reflect the field\u0026rsquo;s progression from narrow, task-specific policies to generalizable robotic foundation models.\nEarly VLAs RT-219 extended RT-1 and coined the term VLA. By adapting pre-trained VLMs, using either PaLI-X20 (55B) or PaLM-E21 (562B) as base architectures. Rather than training robotic policies from scratch, RT-2 finetunes these VLMs on a mixture of web data and robot trajectories, maintaining the original language capabilities while learning robotic control. The main innovation is in representing robot actions as natural language tokens (e.g. [2, 64, 30, 1, 0, 1, 0], for discrete arm and gripper commands), allowing the same transformer architecture to generate both text and robot actions. During robotic tasks, RT-2 constrains its vocabulary to valid action tokens through dynamic vocabulary masking. This approach allowed for compositional reasoning, RT-2 can follow abstract instructions like \u0026ldquo;place the apple next to the coffee mug\u0026rdquo; or \u0026ldquo;move the block onto the number that equals 3*2\u0026rdquo;.\nYour browser does not support the video tag. Figure 5: RT-2 pushing the blue block to the tabasco bottle.\nOpenVLA22 achieved better performance than RT-2\u0026rsquo;s 55B parameter model using only 7B parameters. The model uses a Prismatic-7B vision-language foundation23 based on LLama224 with a fused visual encoder. This encoder combines SigLIP25 (contrastive text-image embeddings similar to CLIP) and DINOv226 (a visual foundation model for patch and class token embeddings) projecting them into LLaMA-2\u0026rsquo;s token space for action prediction. OpenVLA\u0026rsquo;s main innovation is a more efficient action tokenization scheme. While RT-2 represents actions as strings like \u0026ldquo;move_arm(x=0.5, y=0.3, z=0.2, gripper=open)\u0026rdquo;, OpenVLA directly tokenizes continuous action values as numerical tokens: [0.5, 0.3, 0.2, 1.0, 0.0, 0.0, 0.0] corresponding to 3D position, quaternion rotation, and gripper state. This reduces vocabulary overhead and improves training stability. OpenVLA was trained on 970k robot trajectories from the Open X-Embodiment dataset18 spanning 22 different robot embodiments. The model can be fine-tuned using LoRA27 techniques for new tasks and achieves 16.5% better task success rates than RT-2-X across 29 evaluation tasks while running at 6Hz inference speed.\nFigure 6: OpenVLA Architecture. Continuous VLAs All models discussed so far are discrete. The output actions sent to the robot are quantized, typically into 256 bins per action dimension 28. While this quantization makes it easier to debug and validate model outputs, it creates challenges for fine-grained control and smooth motion generation. In contrast, continuous VLAs generate raw motor commands or joint positions directly from visual and language inputs without quantization.\nPhysical Intelligence\u0026rsquo;s $\\pi_{0}$29â€‹ model exemplifies this approach, using flow matching30 to generate 50Hz continuous control signals for dexterous manipulation tasks. Flow matching is a diffusion-inspired generative modeling technique that learns to transform Gaussian noise into structured action trajectories. Unlike diffusion models which require multiple denoising steps, flow matching enables real-time control by directly generating smooth action sequences. $\\pi_{0}$â€‹ uses a hybrid architecture with a 3B parameter VLM based on PaliGemma31 for vision-language processing, and a separate 300 million parameter action expert that handles proprioceptive states and generates continuous actions via flow matching. The model was trained on over 10,000 hours of robot data from 7 different robot platforms across 68 distinct tasks, enabling it to perform complex dexterous manipulation like folding laundry, table bussing, and precise object handling that require the fine motor control impossible with discrete action spaces.\nFigure 7: $\\pi_{0}$ Architecture. Figure AI\u0026rsquo;s Helix model uses a dual-system architecture to address the tradeoff between generalisation and speed in VLA models. System 2 (S2) is a 7B parameter VLM operating at 7-9 Hz for scene understanding and language comprehension, while System (S1) is an 80M parameter cross-attention encoder-decoder transformer that handles low-level control at 200Hz. This seperation allows each system to operate at its optimal timescale. S2 can think slow about high-level goals, while S1 thinks fast to execute precise actions. S2\u0026rsquo;s latent vector is projected onto S1\u0026rsquo;s token space and concatenated with visual features from S1\u0026rsquo;s vision backbone along the sequence dimension, providing task conditioning. This latent vector is a semantic embedding that encodes S2\u0026rsquo;s understanding of the scene and language instruction for S1\u0026rsquo;s motor control. S1 outputs continuous control signals including wrist poses, finger flexion, and abduction control at 200Hz. Helix was trained on approximately 500 hours of teleoperated behaviours and can simultaneously control 2 robots working on shared manipulation tasks. Unfortunately, Figure AI is closed source and outside their blog post there is not a huge amount more information available.\nFigure 8: Helix Dual System Architecture. Efficient VLAs While models like RT-2 and $\\pi_{0}$ demonstrate impressive capabilities, their computational requirements limit practical deployment. A parallel research direction focuses on efficiency optimizationâ€”achieving good performance with fewer parameters and less compute.\nCogACT32 breaks from the monolithic VLM approach used in RT-2 and OpenVLA by separating cognitive and action capabilities into distinct modules. Unlike previous VLAs that adapt vision-language models end-to-end, CogACT uses a dedicated 300M parameter Diffusion Transformer specifically for action modeling, while keeping the Prismatic-7B VLM focused purely on vision-language understanding. This separation allows independent optimization of each component and enables the action module to specialize in temporal action sequences. TinyVLA33 eliminates the pre-training stage entirelyâ€”a departure from the standard VLM robot fine-tuning pipeline. Instead of starting with large pre-trained VLMs like RT-2 or OpenVLA, TinyVLA directly integrates diffusion policy decoders during fine-tuning on robot data, significantly reducing training costs while maintaining performance. SmolVLA34 represents the extreme end of parameter efficiency, using a 450M parameter model with SmolVLM2 as its backbone and a 100M parameter action expert. Designed for consumer-grade hardware, SmolVLA demonstrates that effective robotic control doesn\u0026rsquo;t require massive models. The model was trained exclusively on community-contributed datasets, showing how smaller models can leverage diverse data sources that might be insufficient for larger architectures. Figure 9: SmolVLA Architecture. Limitations and Open Problems Despite remarkable progress, VLA models still face fundamental challenges that constrain deployment beyond controlled laboratory settings. Understanding these limitations is crucial for building reliable robotic systems that can operate safely in the real world.\nData Bottleneck VLA models suffer from a fundamental data scarcity problem that makes their development different from their language model counterparts. While GPT-style models train on billions of text examples scraped from the internet, even the largest robotic datasets remain tiny by comparison. OpenVLA, one of the most trained VLAs, used approximately 970,000 trajectories from the Open X-Embodiment dataset using 22 robot platforms, still orders of magnitude smaller than typical language model training sets.\nThis scarcity stems from the inherent economics of robotic data collection. Unlike text, which exists abundantly online, robotic learning requires physical interaction data that must be generated through expensive human tele-operation or autonomous exploration. Physical Intelligence estimates robotic data collection costs approximately 50 - 100 dollars per hour, compared to pennies for text data. The RT-1 dataset required 17 months of continuous data collection using a fleet of 13 robots to gather 130,000 demonstrations across 700 tasks, a massive undertaking that produced what would be considered a small dataset in the language modeling world. Larger datasets are beginning to emerge however, AgiBot Colloseo35 passes just over one million and invests serious infrastructure to access the datasets.\nThe computational scaling challenges compound this problem. For example, RT-2\u0026rsquo;s 55B parameter model required 64 NVIDIA A100 GPUs running for 15 days to fully train. This would cost approximately $60,000 on most commercial clouds, too much for most university\u0026rsquo;s departments research budgets! This carries over to deployment as well. Unlike language models that can leverage cloud-based inference, real-time robotic control demands low-latency responses that often necessitate running models locally, introducing additional hardware constraints.\nRecent advances in synthetic data generation offer promising but incomplete solutions. NVIDIA\u0026rsquo;s Isaac GR00T Blueprint36 can generate 780,000 synthetic trajectories in 11 hours, equivalent to 6,500 hours of human demonstration data. However, sim-to-real transfer typically sees 50-80% performance drops when moving from simulation to physical hardware. The challenge lies not just in generating realistic physics simulation, but in capturing the subtle environmental variations and edge cases that robots encounter in real-world deployment.\nYour browser does not support the video tag. Figure 10: Isaac GR00T-N1.5-3B in action.\nOne exciting but underexplored idea is the robotics research cloud37, shared facilities with fleets of standardized robots accessible remotely over the internet. Instead of every lab investing hundreds of thousands of dollars on robots that often sit idle between experiments, researchers could pool resources and share access. Teams could upload their VLA policies, run evaluations across diverse manipulation tasks, and collect trajectory data for future training iterationsâ€”all without maintaining their own physical labs. Small-scale versions exist Georgia Tech\u0026rsquo;s Robotarium38, Real Robot Challenge39, but scaling this to manipulation tasks could provide the standardized infrastructure that VLA development needs. This approach could democratise access to robotics by offering robotic training as a service, similar to how cloud providers simplify infrastructure for software development. Helping address what is becoming a growing problem of scale.\nSafety and Reliability in Physical Systems The transition from laboratory demonstrations to real-world deployment exposes critical safety limitations that don\u0026rsquo;t exist in purely digital AI systems. When language models hallucinate, the consequence is typically an incorrect text output. When VLA models hallucinate, robots can perform dangerous actions including collision failures, incorrect force application, or unsafe trajectories that could cause physical damage or human injury.\nVLATest40 recently evaluated several VLAs across 18,604 testing scenes and showed that models often exhibit significant performance degradation under relatively minor environmental changes. Success rates drop dramatically with variations in lighting conditions, camera angles, or obstacle configuration. Models also frequently misidentify target objects when distractors are present, leading to task failures that could be a direct safety risk in production environments.\nThe reliability challenges extend beyond environmental sensitivity to fundamental issues with uncertainty quantification and failure detection41. Current VLA models lack robust mechanisms for recognizing when they are operating outside their training distribution or when their confidence in a particular action sequence is low. Unlike the controlled environments often showcased in VLA papers, real-world deployment introduces countless edge cases and environmental variations that weren\u0026rsquo;t captured in training data.\nRecent commercial deployments highlight both progress and persistent limitations. Physical Intelligence\u0026rsquo;s $\\pi_{0.5}$ model42 operates successfully in rental homes in San Francisco, showing progress of open-world generalisation beyond laboratory settings. Figure AI has certified and deployed their robots in BMWs manufacturing operations.\nThe threat of bad actors is also a concern and research is emerging into VLA adversarial attacks43. VLA models remain susceptible to patch-based attacks44 in both digital and physical settings, where carefully crafted visual perturbations can induce path deviations and disrupt spatial perception. While such attacks might seem academic, they reveal fundamental brittleness in the models\u0026rsquo; visual processing that could be triggered by natural environmental features or wear patterns on equipment.\nGeneralisation and Transfer Learning VLAs represent a significant step toward general-purpose robotic intelligence, yet their deployment beyond controlled laboratory settings reveals fundamental limitations in generalisation and transfer learning. Understanding these challenges, and the emerging solutions to address them, is key to the field\u0026rsquo;s progression toward general robotic systems.\nModern VLAs perform poorly when confronted with scenarios outside their training distribution. OpenVLA completely fails on unseen environments without fine-tuning on each new deployment scenarios. This is a significant problem when considering the diversity of real world environments where robots are expected to operate.\nSeveral promising solutions are emerging to address this challenge. RT-2 demonstrates improved generalisation primarily through exposure to large-scale internet data during training, which provides broader world knowledge. However, the recently released $\\pi_{0.5}$42 model from Physical Intelligence represents more significant progress towards this goal. It achieved the first demonstration of a robot successfully performing complex household tasks in completely unseen homes without any additional training. $\\pi_{0.5}$ accomplishes this through two main innovations:\nHeterogeneous co-training: Rather than training solely on target robot data, $\\pi_{0.5}$ learns from a diverse mixture including mobile robot demonstrations across 100+ homes, data from other robot types, high-level semantic prediction tasks, web-scale vision-language data, and verbal instructions from human supervisors. This varied training regime helps the model develop more robust and transferable representations. A hierarchical reasoning system: Similar to Chain-of-Thought (CoT)45 in LLMs, $\\pi_{0.5}$ first predicts high-level semantic subtasks before generating low-level motor commands. This separation allows the model to leverage different types of knowledge at each level, semantic understanding from web data for high level planning, and precise motor skills from robot demonstrations for execution. Your browser does not support the video tag. Figure 11: $\\pi_{0.5}$ kitchen tasks in a new kitchen.\nI strongly recommend reading the $\\pi_{0.5}$42 paper as it contains some fascinating details about their specific implementations and evaluations.\nSimilar challenges initially plagued cross-embodiment generalisation, where models struggled to transfer skills across different robot bodies. However, recent advancements, particularly with RT-2-X and $\\pi_{0}$, have begun to bridge this gap. These models have shown improved generalisation by primarily scaling up the diversity and volume of training data, allowing them to learn more robust and transferable representations that are less tied to a specific robot\u0026rsquo;s physical form.\nEvaluation for VLAs is still relatively limited compared to LLMs, which is also an area that is lagging. In general VLAs autoregressive nature makes them relatively myopic, they optimise for the immediate next action rather than planning entire sequences. This means they often struggle with more complex reasoning tasks and long-horizon planning. VLABench46, a VLA manipulation simulation evaluation environment, found that over 100 task categories VLAs exhibit a 20% success rate on tasks that required complex reasoning or planning. These results were not compared against $\\pi_{0.5}$ which may have made progress if compared against these.\nThere is still no unified evaluation benchmark for VLA evaluation. VLABench and CALVIN47 are good synthetic benchmarks for general purpose robotic manipulation, and the recently released EmbodiedBench48 looks to offer an exciting range of evaluation tasks. Fundamentally none of these benchmarks are standardised on real robots. Ideally we need a way to evaluate robots performance in the real world on a series of tasks. I suspect we may see something similar to a robot skill test emerging in the next couple of years to evaluate models and validate skills.\nFigure 12: EmbodiedBench\u0026rsquo;s evaluation types. Final Thoughts VLA models represent significant progress towards more capable robotic systems, with companies beginning to heavily invest in developing larger and more capable models. However, the field remains in its infancy. Through researching VLAs for this piece, and my own interests, several questions have emerged that I would be excited to see more research on in the short to medium-term:\nWhat matters in Sim2Real for VLAs? While we understand that VLAs require fine-tuning for specific tasks, we need further insights into what causes failure when transitioning from simulation into the real world. Understanding these failure modes is essential for building robust VLA systems that can operate reliably in practice. How should VLAs compute? Should we optimise for edge deployment with smaller models running directly on robots, or leverage larger, more capable models running in data centers? This choice has important implications for model development and the design of robotic systems themselves. How do we handle VLA hallucination? LLMs have well-established methods for mitigating hallucination, but VLAs present unique challenges. When a robot hallucinates an action plan, the consequences are physical. How do we recover from bad plans or hallucination in VLAs? Can we do something similar to this? How do we achieve efficient data collection and curation for VLAs? Is it better to collect lots of examples of someone completing a task well or to just have a few very high-quality expert demonstrations of that particular task? How do we bridge RL and VLAs for continuous improvement? Traditional robotics has relied heavily on RL for policy optimization. How do we combine the strengths of both approaches? Can we use RL to fine-tune VLA policies for specific tasks while preserving their general capabilities? Or should we develop hybrid architectures where VLAs provide high-level planning while RL handles low-level control? Citation @article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;July\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References T Brown, et al, Language Models are Few-Shot Learners, arXiv:2005.14165, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, Introducing ChatGPT, https://openai.com/index/chatgpt/, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Krizhevsky, I Sutskever, G E Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Robotics Team, Gemini Robotics: Bringing AI into the Physical World, arXiv:2503.20020, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Lambert, J Morrison, V Pyatkin, S Huang, H Ivison, F Brahman, L J V. Miranda, et al, TÃ¼lu 3: Pushing Frontiers in Open Language Model Post-Training, arXiv:2411.15124, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Radford, et al, Learning Transferable Visual Models From Natural Language Supervision, arXiv:2103.00020, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Gong, YA Chung, J Glass, AST: Audio Spectrogram Transformer, arXiv:2104.01778, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Wen, et al, Transformers in Time Series: A Survey, arXiv:2202.07125, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Xu, H Wu, J Wang, M Long, Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy, arXiv:2110.02642, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Chen, K Lu, et al, Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJB Alayrac, J Donahue, P Luc, A Miech, K Simonyan, et al, ðŸ¦©Flamingo: a Visual Language Model for Few-Shot Learning, arXiv:2204.14198, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Lavril, G Izacard, E Grave, G Lample, et al, LLaMA: Open and Efficient Foundation Language Models, arXiv:2302.13971, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpenAI, GPT-4o System Card, arXiv:2410.21276, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGemini Team Google, Gemini: A Family of Highly Capable Multimodal Models, arXiv:2312.11805, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA Brohan, et al, RT-1 Robotics Transformer for Real-World Control at Scale, Robotics: Science and Systems, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM O Torkoglu et al, FiLM-Ensemble: Probabilistic Deep Learning via Feature-wise Linear Modulation, arXiv:2206.00050, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Ryoo, AJ Piergiovanni, A Arnab, M Dehgani, A Angelova, TokenLearner: What Can 8 Learned Tokens Do for Images and Videos?, arXiv:2106.11297, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOpen X-Embodiment Collaboration, Open X-Embodiment: Robotic Learning Datasets and RT-X Models, arXiv:2310.08864, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zitkovich, et al, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control, Proceedings of The 7th Conference on Robot Learning, PMLR 229:2165-2183, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Chen, et al, PaLI-X: On Scaling up a Multilingual Vision and Language Model, arXiv:2305.18565, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Driess, et al, PaLM-E: An Embodied Multimodal Language Model, arXiv:2303.03378v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Kim, K Pertsh, S Karamcheti, et al, OpenVLA: An Open-Source Vision-Language-Action Model, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Karamcheti, S Nair, A Balakrishna, P Liang, T Kollar, D Sadigh, Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models, Proceedings of the 41st International Conference on Machine Learning 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Touvron, T Scialom, et al, Llama 2: Open Foundation and Fine-Tuned Chat Models, arXiv:2307.09288, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nX Zhai, B Mustafa, A Kolesinov, L Beyer, Sigmoid Loss for Language Image Pre-Training, arXiv:2303.15343v4, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Labatut, A Joulin, P Bojanowski, et al, DINOv2: Learning Robust Visual Features without Supervision, arXiv:2304.07193, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE Hu, Y Shen, et al, LoRA: Low-Rank Adaptation of Large Language Models, arXiv:2106.09685, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Wang, H Zhu, M Liu, J Yang, HS Fang, T He, VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers, arXiv:2507.01016, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK Black, et al, $\\pi_{0}$: A Vision-Language-Action Flow Model for General Robot Control\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY Limpan, R Chen, H Ben-Hamu, M Nickel, M Lee, Flow Matching for Generative Modelling, arXiv:2210.02747, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL Beyer, A Steiner, A Pinto, A Kolesnikov, X Wang, X Zhai, et al, PaliGemma: A versatile 3B VLM for transfer, arXiv:2407.07726, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQ Li, Y Liang, Z Wang, et al, CogAct: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, arXiv:2411.19650, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wen, et al, TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation, arXiv:2409.12514, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM Shukor, D Aubakirova, F Capuano, et al. SmolVLA: A vision-language-action model for affordable and efficient robotics, arXiv:2506.01844, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTeam AgiBot-World, AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems, arXiv:2503.06669, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNVIDIA, GR00T N1: An Open Foundation Model for Generalist Humanoid Robots, arXiv:2503.14734, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV Dean, Y G Shavit, A Gupta, Robots on Demand: A Democratized Robotics Research Cloud, Proceedings of the 5th Conference on Robot Learning, PMLR 164:1769-1775, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD Pickem, P Glotfelter, L Wang, M Mote, A Ames, E Feron, M Egerstedt, The Robotarium: A remotely accessible swarm robotics research testbed, International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN GÃ¼rtler, et al, Real Robot Challenge 2022: Learning Dexterous Manipulation from Offline Data in the Real World, Proceedings of the NeurIPS 2022 Competitions Track, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation, Proceedings of the ACM on Software Engineering, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning, International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPhysical Intelligence, $\\pi_{0.5}$ a Vision-Language-Action Model with Open-World Generalization, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE K Jones, et al, Adversarial Attacks on Robotic Vision-Language-Action Models, arXiv, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Cheng, E Xiao, et al, Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models, arXiv:2409:13174, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ Wei, X Wang, D Shuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, arXiv:2201.11903v6, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS Zhang, Z Xu, P Liu, X Yi, X Qiu, et al. VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, arXiv:2412.18194, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nO Mees, L Hermann, E Rosete-Beas, W Burgard, CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks, arXiv:2112.03227v4, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR Yang, H Chen, J Zhang, M Zhao, et al, EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents, Proceedings of the 42 nd International Conference on Machine Learning, 2025.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/modern-approaches/","summary":"\u003cp\u003eIf I could use one word to describe the advancement of ML or AI in the past couple of years it would be \u003cem\u003escale\u003c/em\u003e. When GPT-3\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e was released in 2020 and ChatGPT\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e in 2022, I was impressed with the performance and thought it was essentially a step towards generalisation in Natural Language Processing (NLP), similar to how \u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\"\u003eAlexNet\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e allowed for generalization in image classification. I did not fully grasp the significance Large Language Models (LLMs) would have on robotics and ML in general. The main idea, that I missed, is the significance and relationship of NLP to problems humans have, language is a very expressive format and a key discovery we made by scaling up these LLMs is that by training over internet scale data we have in fact built a very large and expressive model of human sentiment. Sergey Levine \u003ca href=\"https://twimlai.com/podcast/twimlai/%cf%800-a-foundation-model-for-robotics/\"\u003erecently described this\u003c/a\u003e as:\u003c/p\u003e","title":"Robotic Learning Part 4: Modern Approaches"},{"content":"Imagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the reality gap, is a fundamental challenge in robotic learning.\nFigure 1: Example of real-world and simulated environments for training a Kinova Arm. The appeal of simulation is clear: we can attempt thousands of trials in parallel, experiment without risk of spilling coffee or breaking cups, easily reset the simulation to any starting state, and generate unlimited training data. In-fact it is probably safe to say robotic learning as we know it today would be impossible without simulators. But simulations are approximations and can\u0026rsquo;t perfectly capture the physics of gripping a cup, the variations in cup shapes and materials, or the complexities of real-world sensor noise. This creates a problem:\nHow do we ensure that skills learned in simulation transfer effectively to the real world?\nResearchers have developed three main approaches to address this challenge:\nImproving Simulation Fidelity: Making simulations more realistic, so there is less of a mismatch between the policy learned in simulation and in the real-world. Learning Robust Policies: Developing algorithms that are inherently adaptable by accounting for sim-to-real differences during training. Online Adaptation: Enabling policies to efficiently adjust to real-world conditions by online fine-tuning. Making Simulations more Realistic One approach to bridging the reality gap is to design simulators that better match the real world. The intuition behind why this works is straightforward:\nThe smaller the difference between simulation and reality, the smaller the reality gap that must be bridged.\nIf a robot learns to grasp in a highly accurate simulation that captures subtle physical properties like friction coefficients, contact dynamics, and fluid interactions, those skills are more likely to transfer successfully to the real world. However, creating perfect simulations is impossible, there will always be some mismatch with reality. As George Box said, famously:\nAll models are wrong; some are useful. - George Box\nBut which aspect of reality matters most? Most engineers would be familiar with this approach as defining a problems assumptions or boundary conditions before designing a model. For example in grasping tasks, accurate contact dynamics and friction modelling might be essential, whilst precise visual rendering of shadows is less important. In contrast, for vision-based navigation, accurate lighting models could be critical while precise physics are less important.\nSystem Identification System Identification aims to calibrate the parameters within a simulation to match real-world behaviour. This process aims to find the optimal parameters $\\mathbf{\\xi}^{*}$ that minimise the difference between simulated and real trajectories:\n$$ \\mathbf{\\xi}^{*} = \\arg \\min_{\\mathbf{\\xi}} \\sum_{t=1}^{T} || s_{t}^{\\text{real}} - s_{t}^{sim}(\\mathbf{\\xi}) || $$ where $s_{t}^{\\text{real}}$ are real-world observations and $s_{t}^{\\text{sim}}(\\mathbf{\\xi})$ are simulated states using parameters $\\mathbf{\\xi}$.\nThis process generally involves:\nCollecting real robot trajectories and sensor measurements. Selecting simulator parameters (mass, friction coefficients, motor gains, etc) to minimise the difference between the simulated and real-world behaviour. Iteratively refining these parameters as more data becomes available. While system identification is a powerful approach, it poses unique challenges for learned robotics. The parameters we\u0026rsquo;re trying to identify are deeply intertwined with the learning process itself. As a policy learns and explores new regions of the state space, it encounters different dynamic regimes that may require different parameter values for accurate simulation. This creates a chicken-and-egg problem: we need accurate parameters to learn good policies, but we need policies to explore and gather data for parameter identification. Furthermore, learned policies often exploit subtle dynamics that aren\u0026rsquo;t captured by standard physics models, making it difficult to identify parameters that consistently work across the full range of learned behaviours. This is particularly challenging for contact-rich tasks like manipulation, where small parameter errors can lead to drastically different outcomes in both the learning process and final policy behaviour.\nLarger vehicles, such as planes1, trains and automobiles, that may have high order but generally parameterisable and smooth dynamics system id is often used. For more complex robots the non-linear dynamics introduced by the real-world often pose a challenge and can make system id impractical.\nLearned Simulation Rather than manually tuning parameters, learned simulation uses real-world data to improve simulator accuracy directly. The main idea is that while physics-based simulators capture fundamental dynamics well, they often miss subtle effects that are difficult to model analytically. Learning can be used to bridge this gap.\nResidual Dynamics One approach is to learn a residual dynamics model. These models work by combining a base physics model with a learned component that predicts the difference between the simulated and real-world behaviour. Formally, given a base simulator $f_{\\text{sim}}(s_{t}, a_{t})$ and true dynamics $f_{\\text{real}}(s_{t}, a_{t})$, we learn a residual model $f_{\\text{res}}(s_{t}, a_{t})$ such that:\n$$ f_{\\text{real}} \\approx f_{\\text{sim}}(s_{t}, a_{t}) + f_{\\text{res}}(s_{t}, a_{t}). $$This approach2 can be very effective3 because it leverages the prior knowledge of the physics simulator, which is often a far cheaper and easier problem to solve than learning a complete simulator from scratch. For example, in our coffee cup grasping task, the base simulator could handle rigid body dynamics, while the residual learns to correct for joint backlash, motor delays, and complex friction effects.\nDifferentiable Physics In most of the robotic learning approaches discussed so far we assumed the algorithm learns through trial and error. In our coffee cup example this might involve the robot sometimes gripping too hard and crushing the cup, and sometimes gripping too softly and dropping it. After hundreds or thousands of attempts, it should eventually learn a useful grasp strategy.\nImagine instead having a mathematical model that can instantly tell the robot: \u0026ldquo;If you move your finger $2mm$ to the left and reduce gripping force by $4.2\\text{N}$ the cup will be stable in your grasp without being crushed\u0026rdquo;. This is what differentiable physics simulators offer for robotic learning.\nA differentiable physics simulator creates a mathematical model where every physical interaction, can be calculated and, critically, differentiated. This means the robot can compute exactly how small changes in its actions will affect the outcome of grasping the cup.\nUnlike traditional physics engines with non-differentiable components (like discrete collision detection), differentiable simulators express physical laws as continuously differentiable operations. This mathematical property allows for gradient-based optimisation through the entire physical process, effectively letting the robot \u0026ldquo;see into the future\u0026rdquo; to optimise its actions.\n$$ s_{t+1} = f(s_{t}, a_{t}, \\xi). $$ The simulator then provides the Jacobian matrices:\n$$ \\biggl[ \\frac{\\partial s_{t+1}}{\\partial s_{t}}, \\frac{\\partial s_{t+1}}{\\partial a_{t}}, \\frac{\\partial s_{t+1}}{\\partial \\xi_{t}} \\biggr]. $$ These matrices tell us how small changes in the current state, action, or parameters $\\theta$ affect the next state. When optimising over time, BackPropagation Through Time (BPTT) allows gradients to be rolled out for the entire sequence. Enabling the robot to understand how its initial actions influence the final outcome. This is particularly valuable for contact-rich tasks where traditional simulators struggle with discontinuities in the dynamics.\nTo actually learn a policy gradient-based optimisation algorithms are often used including:\nPolicy Optimisation 4, can be used by back-propagating through the simulator: $$ \\nabla_{\\theta}J(\\xi) = \\mathbb{E}_{\\xi \\sim \\Xi} \\bigl[ \\nabla_{\\theta} f(s, a; \\xi) \\bigr]. $$ The gradient of the objective with respect to the policy parameters can be directly computed, rather than relying on purely numerical approximations. MPC w/ Differentiable Shooting5, unlike traditional MPC, which relies on solving an optimisation problem at each time-step, this approach differentiates through the entire trajectory 6 : $$ \\min_{a_{0:T-1}} \\sum_{t=0}^{T-1} c(s_{t}, a_{t}) + c_{T}(s_{T}).\t$$ Trajectory Optimisation, gradient based optimisation techniques like Differential Dynamic Programming (DDP) or iterative Linear Quadratic Regularisation (iLQR) become more powerful with differentiable physics as they can compute the exact derivatives of the dynamics rather than using numerical finite difference methods. Figure 2: DiffTaichi differentiable programming for physical simulation. Recent frameworks likeÂ Brax,Â Nimble, andÂ DiffTaichiÂ implement efficient differentiable physics that integrate seamlessly with deep learning workflows. For robotics applications, differentiable simulation enables more efficient policy learning, automated system identification, and even physics-based perception, where sensor models can be optimised alongside control policies.\nFigure 3: Brax differentiable physics simulator for robotics written in JAX. Domain Randomisation Instead of trying to make the simulation perfect, Domain Randomisation7 (DR) encourages imperfection by training with varying simulation parameters. The main idea is that by exposing the policy to a wide range of simulator variations during training, it will learn to focus on task-relevant features while being robust to variations that don\u0026rsquo;t matter.\nFigure 4: Domain Randomisation was orginially designed with the objective of training an object detector. Mathematically, we can express this as training a policy $\\pi$ to maximise expected performance across a distribution of environments:\n$$ \\pi^{*} = \\arg \\max_{\\pi} \\mathbb{E}_{\\xi \\sim p(\\xi)} [J(\\pi, \\xi)] $$where $\\xi$ represents simulator parameters and $J(\\pi, \\xi)$ is the performance of a policy $\\pi$ in the environment.\nThe main idea is that if we randomise enough aspects of the simulation, the real world becomes one possible outcome among many in the distribution. DR is particularly effective because it naturally produces policies robust to real-world variations, eliminates the need for precise physics modelling and requires no real-world training data.\nFor the coffee cup example, rather than trying to perfectly model the cup DR might vary:\nPhysical Properties: mass, friction. Visual Properties: cup colours, textures, lighting conditions. Sensor Properties: camera noise, force sensor bias. Robot Properties: joint backlash, motor delays. To practically use DR the parameter ranges and distribution types need to be selected carefully. Too broad and the learning process can become inefficient, too narrow and the policy won\u0026rsquo;t be general enough to adapt to the real-world.\nThis challenge has led to advanced techniques like adaptive randomisation (automatically tuning ranges based on performance) and structured randomisation (using domain knowledge to guide parameter variations). The core principle remains:\nBy training across many simulated variations, we can learn policies that transfer to the real world without requiring perfect simulation.\nLearning Strategies for Transfer While improving simulation fidelity helps bridge the reality gap, we can also design learning algorithms that are inherently robust to the sim-to-real transition. Rather than assuming perfect simulation, these approaches focus on learning representations and policies that transfer effectively despite simulation imperfections.\nDomain Adaption Domain adaption8 aims to bridge the sim-to-real gap by teaching robots to recognise and adapt to discrepencies between simulated and real environments. This approach focuses on learning transformations that align the data distributions from both domains. The core idea is simple yet powerful:\nTrain the robot to focus on features that work consistently across both simulation and reality, while ignoring features that differ between them.\nFor instance, the robot should learn that the general shape of a cup is important for grasping, while slight differences in texture or lighting are irrelevant.\nMathematically, domain adaptation works by training neural networks to extract features that minimise the distributional difference between simulation and reality. Formally, given a feature extractor $f_{\\theta}$, we aim to learn features where the distributions match:\n$$ \\min_{\\theta} D \\bigl( f_{\\theta}(x_{sim}) || f_{\\theta}(x_{real}) \\bigr) $$ where $D$ measures the distributional distance, such as KL-divergence.\nThis is often implemented using adversarial training, similar to Generative Adversarial Nets9 (GANs). A discriminator network tries to determine whether features came from simulation or reality, while the feature extractor aims to make this distinction impossible:\n$$ \\min_{\\theta} \\max_{D} \\mathbb{E}_{x_{\\text{sim}}} \\Bigl[ \\log D \\bigl( f_{\\theta}(x_{\\text{sim}}) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$For adversarial domain randomisation, we go a step further by learning a distribution of simulator parameters $p(\\xi)$ that, ideally, produces data indistinguishable from reality:\n$$ \\min_{p(\\xi)} \\max_{D} \\mathbb{E}_{\\xi \\sim p(\\xi)} \\Bigl[ \\log D \\bigl( x_{\\text{sim}}(\\xi) \\bigr) \\Bigr] + \\mathbb{E}_{x_{\\text{real}}} \\Bigl[ 1 - \\log D \\bigl(f_{\\theta} ( x_{\\text{real}}) \\bigr) \\Bigr] . $$In practice, this means our coffee-cup-grasping robot learns representations that work equally well in simulation and reality. When transferred to the real world, the robot focuses on the aspects of cup-grasping that remain consistent, making the sim-to-real transition much smoother.\nThese methods typically require some real-world data, and can be used in a sim-to-real-to-sim10 cycle. In this framework, policies trained in simulation are deployed in the real-world, and the collected data improves the simulation for subsequent iterations. This cyclical approach creates increasingly robust representations with each iteration. Domain adaptation is particularly powerful when combined with other sim-to-real techniques, as it directly addresses the distributional gap while remaining compatible with methods focused on policy robustness or online adaptation.\nFigure 5: REPeat uses a Real2Sim2Real approach to improve robot-assisted feeding. Meta Learning Meta-learning offers an alternative approach to the sim-to-real challenge. Rather than focusing on improving simulator fidelity or training robust policies in simulation, meta-learning takes a fundamentally different approach:\nTrain the robot to quickly adapt to new situations with minimal data.\nThink of it as learning adaptability.\nFor our coffee cup example, instead of training a robot to master grasping a specific cup in simulation (which may not transfer well to reality), meta-learning trains the robot to understand general grasping principles that enable rapid adaptation when encountering real cups with varying properties, textures, and weights using just a few real-world interactions. The emphasis shifts from perfecting the simulation to developing algorithms that can bridge the reality gap through efficient learning.\nMathematically meta-learning can be expressed as a two-level optimisation problem:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(A(\\theta, \\mathcal{T}))] $$where $\\theta$ is a parameterised policy, $p(\\mathcal{T})$ is a distribution over tasks or environments, $A(\\theta, \\mathcal{T})$ is an adaption process that adjusts $\\theta$ for a specific task, and $\\mathcal{L}_{\\mathcal{T}}$ measures the performance on a task $\\mathcal{T}$.\nThis formulation summarises the main idea behind meta-learning, we optimise not for direct task performance but on how well the robot can adapt when facing new situations. For sim-to-real, this can be described as the following process:\n$$ \\begin{align*} \u0026 \\textbf{Meta-Learning for Sim2Real Transfer} \\\\ \u0026 \\\\ \u0026 \\textbf{Initialize:} \\\\ \u0026 \\quad \\text{Meta-parameters: } \\theta \\\\ \u0026 \\quad \\text{Adaptation procedure: } A(\\theta, \\mathcal{D}) \\\\ \u0026 \\quad \\text{Task distribution: } p(\\mathcal{T}) \\text{ over simulation parameters} \\ \\xi \\\\ \u0026 \\\\ \u0026 \\textbf{Simulated Meta-Training:} \\\\ \u0026 \\textbf{for } \\text{iteration} = 1,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Sample batch of tasks } \\{\\mathcal{T}_1,\\dots,\\mathcal{T}_k\\} \\sim p(\\mathcal{T}) \\\\ \u0026 \\quad \\textbf{for each } \\mathcal{T}_i \\textbf{ do:} \\\\ \u0026 \\quad\\quad \\text{Collect simulation trajectories } \\mathcal{D}_i \\\\ \u0026 \\quad\\quad \\text{Split into } \\mathcal{D}^{\\text{train}}_i, \\mathcal{D}^{\\text{test}}_i \\\\ \u0026 \\quad\\quad \\text{Adapt parameters: } \\theta_i = A(\\theta, \\mathcal{D}^{\\text{train}}_i) \\\\ \u0026 \\quad\\quad \\text{Evaluate adapted parameters: } \\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i) \\\\ \u0026 \\quad \\text{Update } \\theta \\text{ to minimize } \\mathbb{E}_{\\mathcal{T}_i}[\\mathcal{L}_{\\mathcal{T}_i}(\\theta_i, \\mathcal{D}^{\\text{test}}_i)] \\\\ \u0026 \\textbf{end for} \\\\ \u0026 \\\\ \u0026 \\textbf{Real-World Deployment:} \\\\ \u0026 \\quad \\text{Collect small real-world dataset } \\mathcal{D}_\\text{real} \\\\ \u0026 \\quad \\text{Adapt to real world: } \\theta_\\text{real} = A(\\theta, \\mathcal{D}_\\text{real}) \\\\ \u0026 \\quad \\text{Deploy adapted policy } \\pi_{\\theta_\\text{real}} \\text{ in real environment} \\\\ \\end{align*} $$In robotics, optimisation based meta-learning approaches have gained the most attention, often based on the Model Agnostic Meta Learning11 (MAML) algorithm. Unlike model-based methods that attempt to learn explicit task dynamics or metric-based approaches that rely on learned distance measures between tasks, MAML directly optimises for adaptability through a gradient-based formulation:\n$$ \\min_{\\theta} \\mathbb{E}_{\\mathcal{T} \\sim p(\\mathcal{T})} [\\mathcal{L}_{\\mathcal{T}}(\\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}}(\\theta))]. $$ For robotic applications, MAML\u0026rsquo;s gradient-based adaptation mechanism integrates naturally with deep learning architectures and standard reinforcement learning objectives. While model-based approaches must learn accurate dynamics models, which can be challenging for complex robotic systems, and metric-based approaches require carefully designed embedding spaces, MAML works directly in parameter space. This allows it to capture sophisticated adaptation strategies without additional architectural constraints.\nFigure 6: ES-MAML uses Evolutionary Strategies (ES) to learn an adaptive control policy for a noisy task. Also, the computation of MAML\u0026rsquo;s adaptation gradientsÂ $\\nabla_{\\theta}\\mathcal{L}_{\\mathcal{T}}(\\theta)$Â can leverage standard automatic differentiation tools, making it easy to implement despite its mathematical sophistication. Often a first-order approximation (FOMAML) is used to improve computational efficiency by ignoring second-order terms in the meta-gradient computation, while still maintaining much of the method\u0026rsquo;s adaptation capabilities.\nWhile MAML provides efficient adaptation through gradient-based updates, it doesn\u0026rsquo;t explicitly model uncertainty in the task parameters, a critical consideration for sim-to-real transfer, where real-world dynamics are initially unknown. Probabilistic meta-learning12 approaches address this limitation by modelling a distribution over possible task parameters:\n$$ p(\\mathcal{T}|\\mathcal{D}) = \\int p(\\mathcal{T}|\\theta) p(\\theta|\\mathcal{D}) d \\theta . $$This allows the robot to maintain and update beliefs about real-world dynamics as it collects data. Probabilistic Embeddings for Actor-Critic RL13 (PEARL) builds on this insight by combining meta-learning with probabilistic inference. Instead of MAML\u0026rsquo;s direct parameter adaptation, PEARL learns a latent space of task variables that capture task uncertainty:\nFigure 7: PEARL\u0026rsquo;s meta-training procedure. $$ \\pi_{\\theta}(a|s, z) \\ \\ \\text{where} \\ \\ z \\sim q_{\\phi}(z|\\mathcal{D}_{\\mathcal{T}}). $$Here, the policyÂ $\\pi_{\\theta}$â€‹Â conditions its actions not just on the current stateÂ $s$, but also on a latent task variableÂ $z$Â inferred from task-specific dataÂ $\\mathcal{D}_{\\mathcal{T}}$â€‹. This structure provides several advantages for sim-to-real transfer:\nThe learned latent space can capture structured uncertainty about task parameters, allowing for more efficient exploration than MAML\u0026rsquo;s gradient-based adaptation. By learning a probabilistic encoderÂ $q_{\\phi}$â€‹, usually via a Variational Auto-Encoder14 (VAE), PEARL can rapidly infer task-relevant parameters from small amounts of real-world data without requiring gradient updates to the policy parameters. This uncertainty-aware approach enables robots to systematically explore and adapt to real-world conditions while maintaining uncertainty estimates about task dynamics. Modular Policy Architectures Rather than treating sim-to-real transfer as a monolithic problem, modular architectures break policies into components that can be transferred or adapted independently. This decomposition allows us to leverage the fact that some aspects of a task may transfer more readily than others. End-to-end systems are also notoriously hard to debug and breaking the problem down into smaller sub-problems can help to identify exactly what part of the system is misbehaving. Robotic tasks often naturally decompose into three main components:\nPerception, understanding the environment through sensors. Planning, deciding what actions to take. Control, precisely executing these actions. Perception modules face domain gaps between clean simulation data and noisy reality. For example, when detecting objects with RGB cameras, simulated images often lack real-world artefacts like motion blur, lens distortion, and varying exposure levels. Some techniques to address this could include:\nUsing synthetic data augmentation with Physically-Based Rendering (PBR) to match real camera characteristics. Implementing CycleGAN-based domain adaptation15 to align synthetic and real image distributions. Applying targeted domain randomisation to critical visual features like lighting and camera parameters. Planning modules need to handle state uncertainty when moving from simulation to reality. Some methods to solve this include:\nUsing belief space planning16 that explicitly considers state uncertainty distributions. Implementing hierarchical17 planning with closed-loop feedback at multiple timescales. Incorporating learned error models18 that predict the magnitude and distribution of real-world deviations from planned trajectories. Control modules must bridge the reality gap in physical interactions. Some methods to solve this include:\nStructured Domain Randomisation19 (SDR), systematically varying physical parameters based on the specific hardware used. This method can also be used for perception problems. Learning-Based Model Predictive Control20 (LBMPC), combining traditional MPC with learned vehicle dynamics. Meta-Learning for Rapid Control Adaptation21. These modular approaches work best when combined with other transfer strategies, like using meta-learning to adapt specific modules or applying domain adaptation selectively. This flexibility in mixing approaches makes modularity a particularly effective tool for bridging the reality gap and can better scale when building robotic systems with a larger team or group where departments need to focus on separate components and end-to-end learning would be infeasible.\nOnline Adaption and Deployment While training in simulation and transfer learning provide essential components for robotic learning, the reality of real-world deployment often presents challenges that cannot be fully anticipated. Environmental variations, hardware differences between robots, and changing task requirements all necessitate real-world adaptation. Online adaptation enables robots to continuously refine their policies during actual deployment, adjusting to real-world conditions that may drift over time or differ from training assumptions.\nThe key challenge in online adaptation is balancing the need for exploration and improvement against maintaining reliable performance and safety. Unlike simulation, where exploration carries no physical risk, real-world adaptation must be conducted carefully to avoid expensive or dangerous failures. This creates a complex trade-off:\nAdapt too conservatively and the robot may never achieve optimal performance, adapt too aggressively and you risks unsafe behaviour.\nModern approaches to online adaptation address this challenge through several complementary strategies. Few-shot adaptation enables rapid policy updates using minimal real-world data. Lifelong learning methods allow robots to accumulate experience while preventing degradation of existing capabilities. Progressive transfer techniques provide structured frameworks for safely transitioning from simulation to real-world operation. Importantly, these approaches must also consider practical deployment constraints like computational resources, hardware variations between robots, and the potential for knowledge sharing across robotic fleets.\nFigure 9: UK online food retailer Ocado\u0026rsquo;s robotic food packing robots. Few-Shot Adaption Online adaptation in robotics often requires making policy adjustments with small quantities of real-world data. Few-shot adaptation techniques address this challenge by enabling rapid policy updates using just a handful of real-world interactions, making them particularly valuable when collecting extensive real-world data is expensive or dangerous. While meta-learning approaches train policies to be inherently adaptable before deployment, few-shot adaptation22 focuses on efficient policy refinement during actual deployment.\nOne strategy, used by SafeAPT23, is to maintain an ensemble of policies trained in simulation, then adapt their combination based on real-world performance:\n$$ \\pi_{\\text{adapted}}(a|s) = \\sum_{i=1}^{N} w_{i}(s) \\pi_{i}(a|s) $$whereÂ $w_{i}(s)$Â is the context-dependent weights updated online using real-world data. This approach allows robots to leverage diverse behaviours, learned in simulation while quickly adapting their mixture to specific operating conditions. The weights can be rapidly updated using techniques like Bayesian inference or online optimisation, requiring only a few real-world samples.\nFigure 8: SafeAPT generates a diverse repertoire of safe policies in simulation, then selects and refines the most suitable policy for real-world goals using a learned safety model. For multi-robot systems, few-shot adaptation24 can be enhanced through shared learning. When one robot successfully adapts to a new situation, its new experience can be validated and shared across the fleet:\n$$ \\mathcal{D}_{\\text{shared}} = \\{ (s, a, r, c)_{i} : V(s, a, c) \u003e \\tau \\} $$where $V(s,a,c)$Â is a validation function that evaluates the safety andÂ performance of state-action pairs under context $c$, and $\\tau$Â is a safety threshold. This allows the fleet to collectively adapt to new situations while maintaining safety guarantees25.\nHardware variations between robots present an additional challenge for few-shot adaptation. One approach is to learn hardware-specific adaptation layers while maintaining a shared base policy:\n$$ \\pi_{\\text{robot}}(a|s) = h_{\\phi}(\\pi_{\\text{base}}(s), \\xi) $$whereÂ $h_{\\phi}$â€‹Â is a hardware-specific adaptation layer andÂ $\\xi$Â represents hardware parameters such as actuator limits, sensor characteristics, and physical dimensions. This architecture allows each robot to quickly adapt to its specific hardware characteristics26 while leveraging shared knowledge.\nAny shared learning framework requires robust validation27 mechanisms. During few-shot learning, runtime monitoring systems can be used to continuously evaluate adapted behaviors against key performance indicators and safety constraints:\n$$ \\text{safe}(s, a) = \\forall i \\in \\{ 1, \\ldots , M \\} : C_{i}(s, a) \\leq 0 $$whereÂ $C_{i}$â€‹Â represent safety constraints. When a robot discovers a promising adaptation, the validation function $V(s,a,c)$ determines whether this experience merits inclusion in the shared dataset $\\mathcal{D}_{\\text{sharedâ€‹}}$. If constraint violations occur during deployment, the system can revert to a known safe policy while collecting data for more robust adaptation. This closed-loop validation approach ensures that the collective learning process remains safe and reliable even as the robot fleet explores new adaptation strategies.\nReal-world examples of fleet learning systems with these validation mechanisms remain scarce in public literature, as they\u0026rsquo;re typically proprietary technologies developed by companies like Waymo, Boston Dynamics, and Amazon Robotics. There is an increasing amount of open-source research for fleet adaptation systems, but these are often limited to small-scale experiments28.\nLifelong Learning While few-shot adaptation handles immediate adjustments, lifelong learning focuses on continuous improvement during extended deployment. This presents a fundamental challenge:\nHow can robots accumulate new knowledge over months or years of operation without forgetting their existing capabilities?\nA key challenge of this trade-off is catastrophic forgetting29. This is particularly important in robotics, where maintaining baseline performance while learning is essential for practical deployment. It is especially challenging in task-agnostic settings where task boundaries are unclear, and the robot must continuously learn without explicit transitions between distinct learning phases that you might have in classical ML setups.\nRegularisation based methods offer one approach to mitigate catastrophic forgetting. Techniques like Elastic Weight Consolidation30 (EWC) identify and protect important parameters for previously learned tasks by adding constraint terms to the loss function:\n$$ \\mathcal{L}_{\\text{EWC}}(\\theta) = \\mathcal{L}_{\\text{current}}(\\theta) + \\sum_{i} \\frac{\\lambda}{2} F_{i}(\\theta - \\theta_{\\text{A, i}}^{*})^{2} $$where $\\mathcal{L}_{\\text{current}}(\\theta)$ represents the loss for the current task, $\\lambda$ describes how important the old task is compared to the new one, and $F_{i}$ is the Fisher information representing parameter importance for task $i$ where $\\theta_{A, i}$ is the optimal parameters for the previous tasks.\nReplay based methods can also be used, such as Prioritized Experience Replay31 (PER), that maintains a buffer of past-experiences $\\mathcal{B}$ with a priority weight $\\alpha(s, a)$. $\\delta(s, a)$ is the temporal difference error that quantifies how much the current policy\u0026rsquo;s predictions deviate from observed rewards and state transitions. The sampling probability is given by:\n$$ P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}} $$where $\\alpha$ determines how much prioritization is used. To correct for sampling bias, importance sampling weights $w_i = (N \\cdot P(i))^{-\\beta}$ are applied to the loss gradients.\nThe learned architecture can also be adjusted to inherently resist forgetting. For example, Progressive Neural Networks32 (PNN) expand the architecture for each new task while preserving previous learned knowledge. PackNet33 partitions network parameters across tasks to prevent interference.\nFor all of these strategies the fundamental challenge remains balancing plasticity (the ability to learn new tasks) with stability (retaining performance on previous tasks). Systems that lean too far toward stability resist new learning, while those prioritizing plasticity risk catastrophic forgetting. Modern approaches often use a blend of these approaches, for example predictive uncertainty estimates34 can be used to decide how samples should be included in the model whilst learning online.\nComplementary to addressing forgetting, efficient memory management is important in the real world. Real robots cannot store petabytes of raw-experience data, and blindly replay all past-experiences as this is simply too expensive and can limit exploration.\nLifelong learning is a complex and rapidly evolving field that deserves more detail than I can provide in this section. As companies scale robotic deployments across more locations with increasingly sophisticated behaviors, I expect we\u0026rsquo;ll discover much more about the specific engineering challenges involved.\nProgressive Transfer Progressive transfer provides a structured approach for transitioning policies from simulation to real-world operation. Rather than attempting an immediate switch, robots gradually reduce their reliance on simulation while building confidence in real-world performance. This approach is particularly important for safety-critical applications and fleet-wide deployments.\nThe core idea usually blends simulation and real-world policies based on deployment confidence:\n$$ a_{\\text{final}}(s,c) = (1-\\beta(s,c))a_{\\text{real}}(s) + \\beta(s,c)a_{\\text{sim}}(s) $$whereÂ $\\beta(s, c) \\in [ 0, 1 ]$ represents confidence in the real-world policy for state $s$ and context $c$. As deployment experience increases and safety metrics improve, $\\beta$ decreases, shifting control from simulation-based to real-world policies. Context $c$ captures task complexity, environmental conditions, and safety requirements.\nSummary I hope this section has helped provide some useful insights into why sim2real is important for real-world robotics. This has proven to be the hardest part of this blog post to write, and I would like to expand in the future on the real-world deployment challenges of the sim2real problem. Just as we have learned that moving ML from the lab to scaled businesses has created its own host of ML problems, I\u0026rsquo;m sure we will continue to see similar challenges with moving robotic learning to scale.\nCitation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;June\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References K W Liff, Parameter Estimation for Flight Vehicles, Journal of Guidance, Control and Dynamics, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN Sontakke, H Chae, S Lee, T Huang, D W. Hong, S Ha, Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots, arXiv:2303.09597, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH Jemin, L Joonho, H Marco, Per-Contact Iteration Method for Solving Contact Dynamics, IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH.J. Terry Suh, Max Simchowitz, Kaiqing Zhang, Russ Tedrake, Do Differentiable Simulators Give Better Policy Gradients?, Proceedings of the 39th International Conference on Machine Learning, PMLR 162, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Romero, E. Aljalbout, Y. Song, D. Scaramuzza, Actor-Critic Model Predictive Control: Differentiable Optimization Meets Reinforcement Learning, arXiv:2306.09852, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Oshin, H. Almubarak, E.A. Theodorou, Differentiable Robust Model Predictive Control, Robotics: Science and Systems, Delft, Netherlands, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World, arXiv:1703.06907, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nY. Ganin, V. Lempitsky, Unsupervised Domain Adaptation by Backpropagation, Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Nets, Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine, R. Hadsell, K. Bousmalis, Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, P. Abbeel, and S. Levine, â€œModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,â€ Proceedings of the 34th International Conference on Machine Learning, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Finn, K. Xu, and S. Levine, â€œProbabilistic Model-Agnostic Meta-Learning,â€ Proceedings of the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, â€œEfficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,â€ Proceedings of the 36th International Conference on Machine Learning (ICML), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ Proceedings of the 2nd International Conference on Learning Representations (ICLR) 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Rao, C. Harris, A. Irpan, S. Levine, J. Ibarz, and M. Khansari, â€œRL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real,â€ Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Patil, G. Kahn, P. Abbeel, and 3 other authors, â€œScaling up Gaussian Belief Space Planning Through Covariance-Free Trajectory Optimization and Automatic Differentiation,â€ Workshop on the Algorithmic Foundations of Robotics (WAFR 2014), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. D. Kulkarni, K. R. Narasimhan, A. Saeedi, and J. B. Tenenbaum, â€œHierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation,â€ Proceedings of the 30th Conference on Neural Information Processing Systems (NeurIPS), Dec. 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Sharma, J. Harrison, M. Tsao, and M. Pavone, â€œRobust and Adaptive Planning under Model Uncertainty,â€ Proceedings of the Twenty-Ninth International Conference on Automated Planning and Scheduling (ICAPS 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State, O. Shapira, and S. Birchfield, â€œStructured Domain Randomization: Bridging the Reality Gap by Context-Aware Synthetic Data,â€ Proceedings of the 2019 International Conference on Robotics and Automation (ICRA), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Hewing, K. P. Wabersich, M. Menner, and M. N. Zeilinger, â€œLearning-Based Model Predictive Control: Toward Safe Learning in Control,â€ Annual Review of Control, Robotics, and Autonomous Systems, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Nagabandi, I. Clavera, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, â€œLearning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning,â€ Proceedings of the 7th International Conference on Learning Representations (ICLR 2019), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nF. Baumeister, L. Mack, and J. Stueckler, â€œIncremental Few-Shot Adaptation for Non-Prehensile Object Manipulation using Parallelizable Physics Simulators,â€ arXiv preprint arXiv:2409.13228, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Kaushik, K. Arndt, and V. Kyrki, â€œSafeAPT: Safe simulation-to-real robot learning using diverse policies learned in simulation,â€ IEEE Robotics and Automation Letters, 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M Bjorkman, D Kragic, \u0026ldquo;Bayesian Meta-Learning for Few-Shot Policy Adaptation across Robotic Platforms\u0026rdquo;, arXiv:2103.03697, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Berducci, S. Yang, R. Mangharam, R. Grosu, \u0026ldquo;Learning Adaptive Safety for Multi-Agent Systems\u0026rdquo;, arXiv:2309.10657v2, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Chen, A. Murali, A. Gupta, \u0026ldquo;Hardware Conditioned Policies for Multi-Robot Transfer Learning\u0026rdquo;, Proceedings of the 32nd Conference on Neural Information Processing Systems (NeurIPS), Montreal, Canada, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Garg, S. Zhang, O. So, C. Dawson, Chuchu Fan, \u0026ldquo;Learning Safe Control for Multi-Robot Systems: Methods, Verification and Open Challenges\u0026rdquo;, arXiv:2311.13714v1, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Muller, S. Brahmbhatt, A. Deka, Q Leboutet, D. Hafner, V. Koltun, \u0026ldquo;OpenBot-Fleet: A System for Collective Learning with Real Robots\u0026rdquo;, arXiv:2405.07515v1, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. French, \u0026ldquo;Catastrophic Forgetting in Connectionist Networks\u0026rdquo;, Trends in Cognitive Sciences, 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Kirkpatrick, R. Pascanu, Neil C. Rabinowitz, J. Veness, G. Desjardins, A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R, Hadsell, \u0026ldquo;Overcoming catastrophic forgetting in neural networks\u0026rdquo;, arXiv:1612.00796v2, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Schaul, J. Quan, I. Antonoglou, D. Silver, \u0026ldquo;Prioritized Experience Replay\u0026rdquo;, International Conference on Learned Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, R. Hadsell, \u0026ldquo;Progressive Neural Networks\u0026rdquo;, arXiv:1606.04671, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Mallya, S. Lazebnik, \u0026ldquo;PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\u0026rdquo;, arXiv:1711.05769, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Serra, B. Werner, F. Buettner, \u0026ldquo;How to Leverage Predictive Uncertainty Estimates for Reducing Catastrophic Forgetting in Online Continual Learning\u0026rdquo;, Proceedings of 3rd Workshop on Uncertainty Reasoning and Quantification in Decision Making, UDM-KDD, 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/the-reality-gap/","summary":"\u003cp\u003eImagine teaching a robot to pick up a coffee cup in a simulation or video game. In this perfect virtual world, the cup\u0026rsquo;s weight is precisely known, the lighting is consistent, and the robot\u0026rsquo;s sensors provide exact measurements. Now try the same task in the real world. The cup might be heavier than expected, it\u0026rsquo;s surface more slippery, the lighting creating unexpected shadows, and the robot\u0026rsquo;s sensors noisy. This disconnect between simulation and reality, known as the \u003cem\u003ereality gap\u003c/em\u003e, is a fundamental challenge in robotic learning.\u003c/p\u003e","title":"Robotic Learning Part 3: The Reality Gap"},{"content":"In this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\nImitation Learning: Teaching robots by showing them what to do Reinforcement Learning: Letting robots discover solutions through experience Supervised Learning: Using labeled data to build core perception and planning capabilities Each of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a pick-and-place task (similar to our coffee cup examples) using each of the methods discussed. These scripts are available on GitHub at RLFoundations. Due to the natural challenges and computational expense of robotic learning, this repository also includes pre-trained models that can be downloaded from Hugging Face. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\nImitation Learning Imagine trying to exactly describe to someone how to pickup a coffee cup. Try describing exactly how to pick up the cup, accounting for every finger position, force applied, and possible cup variation. It would be almost impossible, it is far easier to simply show someone how to pick up a coffee cup and have them watch you. This intuition, that some tasks are better shown than described, is the core idea behind Imitation Learning (IL).\nThe Main Challenge At first glance, IL may seem straightforward: show the robot what to do, and have it copy those actions. The main problem is even if we demonstrate the task perfectly hundreds of times the robot needs to generalise across various initial conditions, in our coffee cup example this could be:\nDifferent cup positions and orientations Varying lighting conditions Different cup sizes, shapes and materials Different table heights and surface materials IL isn\u0026rsquo;t just about copying demonstrations exactly, it is about extracting the underlying logic that makes the task successful. This generally follows a sequential process of:\nCollect demonstrations Learn a mapping from states to actions that captures underlying behaviour Handle generalisation by fine-tuning to unseen demonstrations online. Collecting demonstrations The first question that arises is how to generate samples that can be used for training, these will generally be task and user specific, some common examples include:\nTeleoperation Teleoperation1 lets operators control robots remotely via VR controllers and joysticks, enabling safe data collection and precise control while protecting operators. However, interface limitations like latency and reduced sensory feedback can restrict the operator\u0026rsquo;s ability to perform complex manipulations.\nYour browser does not support the video tag. Figure 1: NVIDIA Groot, teleoperation of a humanoid robot.\nKinesthetic Demonstrations Kinesthetic2 teaching enables operators to physically guide robot movements by hand, providing natural and intuitive demonstrations of desired behaviours. While particularly effective for teaching fine-grained manipulation tasks, this method is limited by physical accessibility requirements and operator fatigue.\nYour browser does not support the video tag. Figure 2: Wood Planing, kinesthetic programming by demonstration (Alberto Montebelli, Franz Steinmetz and Ville Kyrki Intelligent Robotics - Aalto University, Helsinki).\nThird Person Demonstrations Third-person demonstrations capture human task execution through video recording, allowing efficient collection of natural behavioural data. However, translating actions between human and robot perspectives creates challenges in mapping movements accurately. Ego4D3, Epic Kitchens 4 and Meta\u0026rsquo;s Project Aria (shown below) are examples of this.\nYour browser does not support the video tag. Figure 3: Meta Project Aria (Dima Damen - University of Bristol).\nLearning from Demonstrations Once we have collected a dataset of demonstrations we need to learn a policy from them. Formally given an expert policy $\\pi_{E}$ used to generate a dataset of demonstrations $\\mathcal{D}={(s_{i},a_{i})}^{N}_{i=1}$, where $s_{i}$ represents states and $a_{i}$ is the experts actions, the objective of IL is to find a policy $\\pi$ that approximates $\\pi_{E}$, such that:\n$$ \\pi^* = \\arg\\min_{\\pi} \\mathbb{E}_{(s,a) \\sim \\mathcal{D}} \\big[ \\mathcal{L}(\\pi(a|s), \\pi_E(a|s)) \\big] $$ where $\\mathcal{L}$ is a loss function measuring the discrepancy between the learned policy $\\pi$ and the expert policy $\\pi^{*}$.\nBehaviour Cloning5 (BC) The simplest approach to imitation learning is simply to treat it as a supervised learning problem. Given demonstrations $\\tau=(s_{t},a_{t})$, BC directly learns a mapping $\\pi_{\\theta}(s)\\rightarrow a$ by minimising:\n$$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\mathbb{E}_{(s, a) \\sim \\tau} [|| \\pi_{\\theta}(s) - a ||^{2}] $$ Figure 4: BC training process. Demonstrations are initially collected using the oracle $\\pi_{E}$ and then trained using supervised learning based on this dataset. The main problem with pure BC is distributional shift, where small errors accumulate over time as the policy encounters states unseen during training.\nGenerative Adversarial Imitation Learning6 (GAIL) GAIL frames IL as a distributional matching problem between policy and expert trajectories using adversarial learning GAIL learns:\nA discriminator $D$ that aims to distinguish between expert and policy generated state-action pairs. A policy $\\pi$, trained to maximise the discriminator confusion. GAIL\u0026rsquo;s optimisation objective is written as:\n$$ \\min_{\\pi} â€‹\\max_{â€‹D} \\mathbb{E}_{\\pi}â€‹[\\log(D(s_{t}, a_{t}))]+\\mathbb{E}_{\\pi_{E}}â€‹[\\log(1âˆ’D(s_{t},a_{t}))]âˆ’\\lambda H(\\pi) $$where $H(\\pi)$ is a policy entropy regularization term for exploration.\nFigure 5: GAIL training process. The dataset $\\mathcal{D}$ is initialized with data from the expert policy $\\pi_{E}$, data generated by the adversary is labelled $(s_{t}, a_{t})_{1}$ and $(s_{t}, a_{t})_{0}$ from the policy $\\pi_{\\theta}$. Dataset Aggregation7 (DAgger) DAgger aims to address distributional shift by iteratively collecting corrective demonstrations, this can be written as:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Train } \\pi_1 \\text{ on expert demonstrations } \\mathcal{D}_0 \\\\ \u0026 \\textbf{for } i = 1,2,\\dots,N \\textbf{ do:} \\\\ \u0026 \\quad \\text{Execute } \\pi_i \\text{ to collect states } \\{s_1, s_2, \\dots, s_n\\} \\\\ \u0026 \\quad \\text{Query expert for labels: } \\mathcal{D}_i = \\{(s, \\pi_{E}(s))\\} \\\\ \u0026 \\quad \\text{Aggregate datasets: } \\mathcal{D} = \\bigcup_{j=0}^i \\mathcal{D}_j \\\\ \u0026 \\quad \\text{Train } \\pi_{i+1} \\text{ on } \\mathcal{D} \\text{ using supervised learning} \\\\ \u0026 \\textbf{end for} \\end{align*} $$The key problem with DAgger is the need for access to an oracle/expert online to query for expert labels. Variants of Dagger aim to address this and other problems by:\nSelectively querying the expert when confidence is low ThriftyDagger8 Using filters to prevent the agent executing dangerous actions SafeDAgger9 Using cost-to-go estimates to improve long-term horizon decision making AggreVaTe10 Reinforcement Learning While IL relies on demonstrations to teach robots, Reinforcement Learning (RL) takes a fundamentally different yet complementary approach - learning through direct interaction with the environment. Rather than mimicking expert behaviour, RL enables robots to discover optimal solutions through trial and error guided by reward signals.\nProblem Definition RL formalises the learning problem as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \\gamma)$ where:\n$S$ is the state space (e.g., joint angles, end-effector pose, visual observations). $A$ is the action space (e.g., joint velocities, motor torques). $P(s_{t+1}|s_{t},a_{t})$ defines the transition dynamics. $R(s_t,a_t)$ provides the reward signal. $\\gamma \\in [0,1]$ is a discount factor for future rewards. The goal is to learn a policy $\\pi(a|s)$ that maximises the expected sum of discounted rewards:\n$$ J(\\pi)=\\mathbb{E}_{\\tau \\sim \\pi} \\biggl[ \\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t},a_{t} ) \\biggr] . $$The Main Challenge Using our coffee cup example, rather than showing the robot how to grasp, we specify a reward signal, perhaps +1 for a successful grasp and 0 otherwise. This seemingly simple shift introduces several key challenges:\nExploration vs Exploitation, a robot learning to grasp cups faces a crucial tradeoff: Should it stick with a mediocre but reliable grasp strategy, or try new motions that could either lead to better grasps or costly failures? Too much exploration risks dropping cups, while too little may prevent discovering optimal solutions.\nCredit Assignment, when a grasp succeeds, which specific actions in the trajectory were actually crucial for success? The final gripper closure, the approach vector, or the pre-grasp positioning? The delayed nature of the reward makes it difficult to identify which decisions were truly important.\nThe Reality Gap between simulation and real-world training. While we can safely attempt millions of grasps in simulation, transferring these policies to physical robots faces numerous challenges:\nImperfect physics modelling of contact dynamics Sensor noise and delays not present in simulation Real-world lighting and visual variations Physical wear and tear on hardware These fundamental challenges have driven the development of various RL approaches that we\u0026rsquo;ll explore in the following sections, from model-based methods that learn explicit world models to hierarchical approaches that break down complex tasks into manageable sub-problems.\nModel-Free RL Model-free methods learn directly from experience, attempting to find optimal policies through trial and error without explicitly modelling how the world works. They can be broadly categorised through three approaches:\n1. Value-Based Methods These approaches learn a value function $Q(s,a)$ that predicts the expected sum of future rewards for taking action $a$ in state $s$. The policy is then derived by selecting actions that maximise this value:\n$$ \\pi(s) = \\arg\\max_{a} Q(s,a) . $$The classic example is DQN11, which uses neural networks to approximate Q-values and was initially trained on Breakout. Value-based methods work well in discrete action spaces but struggle with continuous actions common in robotics, as maximising $Q(s,a)$ becomes an expensive optimisation problem.\nFigure 6: Deep-Q learning with replay buffer. The agent samples mini-batches from the replay buffer to update the critic network $Q_{\\phi}$, while the target network $Q_{\\phi}^{T}$ is periodically updated to stabilize the training. 2. Policy Gradient Methods Rather than learning values, these methods directly optimise a policy $\\pi_{\\theta}(a|s)$ to maximise expected rewards:\n$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) R(\\tau) \\biggr] $$Policy gradients can naturally handle continuous actions and directly optimise the desired behaviour. However, they often suffer from high variance in gradient estimates, leading to unstable training. This high variance occurs because the algorithm needs to estimate expected returns using a limited number of sampled trajectories, and the correlation between actions and future returns becomes increasingly noisy over long horizons.\nSeveral key innovations have been proposed to address this variance problem:\nBaselines: Subtracting a state-dependent baseline $b(s)$ from returns reduces variance without introducing bias:$$ \\nabla_{\\theta} J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\biggl[ \\sum_{t=0}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t}|s_{t}) (R(\\tau) - b(s_t)) \\biggr].$$ Advantage estimation12 : Instead of using full returns, we can estimate the advantage $A(s,a) = Q(s,a) - V(s)$ of actions to reduce variance while maintaining unbiased gradients. Trust regions13 : TRPO constrains policy updates to prevent destructively large changes by enforcing a KL divergence constraint between old and new policies. PPO\u0026rsquo;s clipped objective14 : Simplifies TRPO by clipping the policy ratio instead of using a hard constraint, providing similar benefits with simpler implementation. These improvements have made policy gradient methods far more practical for robotic learning, though they still typically require more samples than value-based approaches.\nFigure 7: Policy gradient update with replay buffer. The agent stores transition tuples $(s_{t}, a_{t}, r_{t})$ in the buffer and samples mini-batches to update the policy, optimizing actions $a_{t}$ for given state $s_{t}$. 3. Actor-Critic Methods Actor-critic methods combine the advantages of both approaches:\nAn actor (policy) $\\pi_\\theta(a|s)$ learns to select actions. A critic (value function) $Q_\\phi(s,a)$ evaluates those actions. These methods aim to address key limitations of both value-based and policy gradient approaches. Value-based methods struggle with continuous actions common in robotics, while policy gradients suffer from high variance and sample inefficiency. Actor-critic methods tackle these challenges by using the critic to provide lower-variance estimates of expected returns while maintaining the actor\u0026rsquo;s ability to handle continuous actions.\nSoft Actor-Critic15 (SAC) represents the state-of-the-art in this family, and makes use of several key innovations:\nThe Maximum Entropy Framework forms the theoretical foundation of SAC, augmenting the standard RL objective with an entropy term. This modification trains the policy to maximise both expected return and entropy simultaneously, automatically trading off exploration vs exploitation. Compared to traditional exploration methods like $\\epsilon$-greedy or noise-based approaches, this framework provides greater robustness to hyperparameter choices and enables the discovery of multiple near-optimal behaviors, ultimately leading to better generalization. Double Q-Learning with Clipped Critics16, actor-critic methods have a tendency to overestimate the value of the Q-function, leading to suboptimal policies. SAC addresses this by using two Q-functions and taking the minimum of their estimates to reduce overestimation bias and preventing premature convergence. The Reparameterisation Trick17 improves policy optimization by making the action sampling process differentiable. The policy network outputs the parameters $(\\mu, \\sigma)$ from a Gaussian distribution over actions, and actions are sampled from the reparameterisation $a = \\mu + \\sigma \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,1)$. This allows for direct backpropagation through the policy network, reducing variance in gradient estimates and improving training stability. The complete for SAC objective becomes:\n$$ J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t,a_t) + \\alpha H(\\pi(\\cdot|s_t)))\\right] $$where $H(\\pi(\\cdot|s_t))$ is the entropy of the policy and $\\alpha$ balances exploration with exploitation.\nFigure 8: Actor-Critic update with Advantage Estimation and replay buffer. The actor $\\pi_{\\theta}$ updates its policy using the advantage estimate, $A^{\\pi}(s_{t}, a_{t}) = Q^{\\pi}(s_{t}, a_{t}) - V^{\\pi}(s_{t})$. The target network $Q_{\\phi}^{T}$ stabilizes learning by providing periodic updates to the critic. SAC has become the preferred choice for robotic learning18 because it:\nLearns efficiently from off-policy data Automatically adjusts exploration through entropy maximisation Provides stable training across different hyperparameter settings Achieves state-of-the-art sample efficiency and asymptotic performance Model-Based RL (MBRL) Model-based RL aims to improve sample efficiency by learning a dynamics model of the environment and using it for planning or policy learning. The key idea is that if we can predict how our actions affect the world, we can learn more efficiently from limited real-world data.\nThe core idea of MBRL can be broken down into three key components:\nData Collection: interact with the environment to collect trajectories Model Learning: Train a dynamics model to predict state transitions Policy Optimisation: Use the model to improve the policy through planning or simulation Ideally this begins a cycle where better models lead to be to better policies, which in turn collect better data.\nLearning the Dynamics Model Given collected transitions we need to learn a function $f_\\theta$ that predicts how our actions change the world:\n$$ \\hat{s}_{t+1} = f_\\theta(s_t, a_t) \\approx P(s_{t+1}|s_t,a_t) $$For robotic tasks, this model can take two forms:\nDeterministic Models: Directly predict the next state, like if I close the gripper by 2cm, the cup will move up by 5cm.\nProbabilistic Models: Capture uncertainty in predictions:\n$$ P(s_{t+1}âˆ£s_{t},a_{t})=\\mathcal{N} \\bigl( \\mu_{\\theta}(s_{t},a_{t}),\\Sigma_{\\theta}(s_{t},a_{t}) \\bigr) $$For example, predicting closing the gripper has a 90% chance of stable grasp, 10% chance of knocking the cup over. This type of modelling has proven to be useful for safe learning.\nOnce we have a dynamics model, there are two fundamentally different approaches:\nPlanning-Based Control Planning methods use the model to simulate and evaluate potential future trajectories. The two main approaches are:\nModel Predictive Control19 (MPC) repeatedly solves a finite-horizon optimisation problem at each time-step:\n$$ a_{t:t+H}â€‹=\\arg\\max_{a_{t:t+H}}â€‹ \\sum_{h=0}^{H} â€‹r(s_{h}â€‹,a_{h}â€‹) \\ \\text{where} \\ s_{h+1}â€‹=f_{\\theta}â€‹(s_{h}â€‹,a_{h}â€‹) $$This optimisation problem is often solved using a sampling-based approaches like Cross-Entropy Method (CEM) or Covariance Matrix Adaptation Evolution Strategy (CMA-ES) which are often favored because they are easily parallelisable on GPUs and can optimise nonlinear, high-dimensional action spaces without requiring derivatives of the cost function. These methods iteratively sample and refine candidate action sequences, making them well-suited for complex control tasks. The general MPC process at each time step $t$ is:\nGenerate $K$ action sequences: $$\\{a_{t:t+H}^{(k)}\\}_{k=1}^{K}$$ Simulate trajectories using model: $s_{h+1}^{(k)} = f_{\\theta}(s_h^{(k)}, a_h^{(k)})$. Execute first action of the best sequence: $$ a_t = a_{t:t+H}^{(k)}[0]$$ where $$k^{*} = \\arg\\max_k \\sum_{h=0}^{H} r(s_h^{(k)}, a_h^{(k)}).$$ Figure 9: Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Black dots represent sampled candidate solutions, while the orange ellipses illustrate the evolving covariance matrix. The algorithm progressively refines its distribution toward the global minima as variance reduces. Gradient-Based Planning methods use the differentiability of both the learned dynamics model $f_{\\theta}$ and the reward function $r(s_{h}, a_{h})$ to compute the gradient of the expected return with respect to the action sequence $a_{t:t+H}$, enabling direct optimisation through gradient descent. Compared to sampling based methods by following the gradient of expected return the planner can rapidly converge to high-value action sequences without extensive random sampling. This is both more computationally efficient precise than sampling based methods. As the continuous optimisation space offers results in more accurate actions for fine control outputs.\nMethods like PETS20 optimise action sequences directly through gradient descent on the expected return:\n$$ J(a_{t:t+H}) = \\mathbb{E}_{s_{h+1} \\sim f_{\\theta}(s_{h}, a_{h}}) \\biggl[ \\sum_{h=0}^{H} r(s_{h}, a_{h}) \\biggr] $$$$ a_{t:t+H}^{*} = \\arg \\max_{a_{t:t+H}} J(a_{t:t+H}) $$Building on this Dreamer extends gradient-based planning to latent space, where it learns a world model that can be efficiently differentiated through time. By planning in a learned latent space, rather than raw observations, Dreamer can handle high-dimensional inputs whilst maintaining the computational benefits of gradient-based optimisation.\nFigure 10: Dreamer recurrent world model with an encoder-decoder structure. The model predicts latent states $z_{t}$ from observations $x_{t}$, generating reconstructions $\\hat{x}_{t}$. The recurrent module $h_{t}$ captures temporal dependencies, while the model uses latent dynamics to predict future states and inform actions $a_{t}$. The main problem with all of these methods is how they deal with non-differentiable dynamics or discontinuous rewards, which can lead to sparse optima or unstable gradients. These problems can be addressed with methods like smoothing functions or robust optimisation, but this naturally adds more engineering effort and can harm performance.\nModel-Based Policy Learning Rather than planning actions online, an alternative approach is to leverage the learned dynamics model to train a policy through simulated experiences. This approach combines the sample efficiency of model-based methods with the fast inference of model-free policies.\nDynastyle Algorithms21 mix real and simulated data for policy updates. By mixing experiences from both sources, these methods balance the bias-variance trade-off between potentially imperfect model predictions and limited real-world data. This objective becomes:\n$$ J( \\pi_{\\phi}) = \\alpha \\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{real}}} [Q(s, a)] + (1-\\alpha)\\mathbb{E}_{(s, a) \\sim \\mathcal{D}_{\\text{model}}} [Q(s, a)] $$where $\\mathcal{D}_{\\text{real}}$ is collected from the real environment and $\\mathcal{D}_{\\text{model}}$ is generated using the learned model $f_{\\theta}$. The mixing coefficient $\\alpha$ controls the trade-off between real and simulated data.\nModel Based Policy Optimisation22 (MBPO) addresses the challenge of compounding prediction errors in learned dynamics models by limiting synthetic rollouts to short horizons. The main insight is that although learned models become unreliable for long-term predictions, they remain accurate for short-term forecasting, making them valuable for generating high-quality synthetic data. To ensure reliability MBPO incorporates two mechanisms to handle two types of uncertainty:\nAleatoric Uncertainty is randomness inherent to the enviornment that cannot be reduced by collecting larger quantitys of data. To account for this MBPO models transitions as probabilistic distributions rather than fixed outcomes. Each network outputs a Gaussian distribution over possible next states: $$ p_\\theta^i(s_{t+1}|s_t,a_t) = \\mathcal{N}\\bigl(\\mu_\\theta^i(s_t,a_t), \\Sigma_\\theta^i(s_t,a_t)\\bigr) $$ Epistemic Uncertainty, is uncertainty in the model itself and comes from limited or biased training data and can be reduced with better model learning. MBPO handles epistemic uncertainty via an ensemble of models $(p_\\theta^1,\u0026hellip;,p_\\theta^B)$. During synthetic rollouts, one model is randomly selected for each prediction. This approach ensures that predictions reflect the range of plausible dynamics, avoiding overconfidence in poorly understood regions of the state space. The algorithm can be summarized as follows:\n$$ \\begin{align*} \u0026 \\textbf{Initialize: } \\text{Policy: } \\pi_\\phi, \\text{ Model Ensemble: } \\{p_\\theta^1,...,p_\\theta^B\\}, \\text{ Replay Buffers: } \\{ \\mathcal{D}_\\text{env}, \\mathcal{D}_{\\text{model}} \\} \\\\ \u0026 \\textbf{for } N \\text{ epochs do:} \\\\ \u0026 \\quad \\text{for } E \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\text{Take action in environment: } a_t \\sim \\pi_\\phi(s_t) \\\\ \u0026 \\quad \\quad \\text{Add to replay buffer: } \\mathcal{D}_\\text{env} \\leftarrow \\mathcal{D}_\\text{env} \\cup \\{(s_t, a_t, r_t, s_{t+1})\\} \\\\ \u0026 \\quad \\text{for } i = 1,\\dots,B \\text{ do:} \\\\ \u0026 \\quad \\quad \\text{Train } p_\\theta^i \\text{ on bootstrapped sample from } \\mathcal{D}_\\text{env} \\\\ \u0026 \\quad \\text{for } M \\text{ model rollouts do:} \\\\ \u0026 \\quad \\quad s_t \\sim \\mathcal{D}_\\text{env} \\text{ // Sample real state} \\\\ \u0026 \\quad \\quad \\text{for } k = 1,\\dots,K \\text{ steps do:} \\\\ \u0026 \\quad \\quad \\quad a_{t+k} \\sim \\pi_\\phi(s_{t+k}) \\\\ \u0026 \\quad \\quad \\quad i \\sim \\text{Uniform}(1,B) \\text{ // Sample model from ensemble} \\\\ \u0026 \\quad \\quad \\quad s_{t+k+1} \\sim p_\\theta^i(s_{t+k+1}|s_{t+k}, a_{t+k}) \\\\ \u0026 \\quad \\quad \\quad \\mathcal{D}_\\text{model} \\leftarrow \\mathcal{D}_\\text{model} \\cup \\{(s_{t+k}, a_{t+k}, r_{t+k}, s_{t+k+1})\\} \\\\ \u0026 \\quad \\text{for } G \\text{ gradient updates do:} \\\\ \u0026 \\quad \\quad \\phi \\leftarrow \\phi - \\lambda_\\pi \\nabla_\\phi J_\\pi(\\phi, \\mathcal{D}_\\text{model}) \\\\ \u0026 \\textbf{end for} \\end{align*} $$Where:\n$K$ is the model rollout horizon $f_\\theta$ is an ensemble of probabilistic neural networks $J_\\pi$ is the policy optimization objective (often SAC) $\\lambda_\\pi$ is the learning rate In practice, MBPO has proven particularly effective for robotic control tasks, where collecting real-world data is expensive.\nChallenges in MBRL MBRL faces several fundamental challenges that make it particularly difficult in robotics:\nCompounding Model Errors, are a significant problem in MBRL. A small error in predicting finger position at $t=1$ results in slightly incorrect contact points, which leads to larger errors in predicted contact forces at $t=2$. By $t=10$, the model might predict a successful grasp while in reality the cup has been knocked over. This error accumulation can be expressed formally, given a learned model $f_{\\theta}$, this prediction error grows approximately exponentially with horizon $H$:\n$$||\\hat{s}_{H} - s_{H}|| \\approx \\|\\nabla f_{\\theta}\\|^H \\|\\epsilon\\|$$where $\\epsilon$ is the one-step prediction error.\nReal-World Physics presents significant challenges due to its discontinuous nature, especially during object interactions and contacts. Learned models struggle to capture these discontinuities because they must simultaneously handle two distinct regimes: continuous dynamics in free space and discontinuous dynamics during contact. Additionally, the system exhibits high sensitivity to initial conditions, where microscopic variations in parameters like surface friction can lead to macroscopically different outcomes, for instance, determining whether a gripper maintains or loses its grasp on an object. These abrupt transitions between physical states and the sensitive dependence on initial conditions make it particularly challenging to learn and maintain accurate predictive models.\nSupervised Learning A key question in designing robotic systems is whether to pursue an end-to-end approach that learns directly from raw sensory inputs to actions, or decompose the problem into modular components that can be trained independently. End-to-end learning offers the theoretical advantage of learning optimal task-specific representations and avoiding hand-engineered decompositions. The main idea is that by training the entire perception-to-action pipeline jointly, the system can learn representations that are optimally suited for the task.\nWhilst appealing in theory, end-to-end learning faces several practical challenges in real robotics. End-to-end systems typically require vast quantities of task-specific data, as they must learn everything from scratch for each new task. They also tend to be brittle, a change in lighting conditions or robot configuration might require retraining the entire system. But perhaps the most significant challenge is the lack of interpretability, end-to-end systems are often described as black boxes because it is difficult to understand how they arrive at their decisions. This makes it hard to diagnose failures or understand why the system behaves in a particular way.\nIn contrast, modular approaches break down the robotic learning problem into specialized components - typically perception, state estimation, planning, and control. Each module can be trained independently using techniques best suited for its specific challenges. This decomposition offers several key advantages:\nInterpretability: Each module can be understood and debugged independently, making it easier to diagnose failures and understand the system\u0026rsquo;s behavior. Reusability: Modules can be reused across different tasks, reducing the need for task-specific data and speeding up development. Robustness: By breaking the problem into smaller, more manageable components, modular systems tend to be more robust to changes in the environment or robot configuration. Sample Efficiency: By training each module independently, modular systems can leverage domain-specific knowledge and data, reducing the need for vast quantities of task-specific data. While IL and RL focus on learning behaviours, Supervised Learning (SL) forms the backbone of many fundamental robotic capabilities. In our coffee cup example, before a robot can even attempt to grasp, it needs to:\nDetect and locate cups in its visual field Estimate the cup\u0026rsquo;s pose and orientation Predict stable grasp points Track its own gripper position These perception and state estimation tasks can be handled through supervised learning. Some common SL tasks in robotics include:\nVisual Perception Modern robotic systems heavily rely on deep learning for visual perception tasks. Convolutional Neural Networks (CNNs) have revolutionized computer vision, enabling robots to understand complex visual scenes and make decisions based on them based on raw pixels alone. There are several common computer vision tasks in robotics:\nObject Detection enables robots to identify and localize objects in their environment. Modern architectures have evolved from two-stage detectors like Faster R-CNN, which use Region Proposal Networks (RPN) for high accuracy, to single-stage detectors like YOLO v8 that achieve real-time performance crucial for reactive robotic systems. Recent transformer-based approaches like DETR23 have revolutionized the field by removing hand-crafted components such as non-maximum suppression, while few-shot detection methods like DeFRCN24 enable robots to learn new objects from limited examples. These advances directly address critical robotics challenges including: real-time processing requirements, handling partial occlusions in cluttered environments, and adaptation to varying lighting conditions. Your browser does not support the video tag. Figure 11: YOLO-NAS object detection.\nSemantic Segmentation provides robots with pixel-wise scene understanding, enabling precise differentiation between objects, surfaces, and free space. State-of-the-art approaches like DeepLabv3+25 and UNet++26 provide high-resolution segmentation maps, while efficient architectures like FastSCNN27 enable real-time performance necessary for robot navigation. The emergence of transformer-based models like the Segment Anything Model28 (SAM) has pushed the boundaries of segmentation capability, especially for handling novel objects and complex scenes. Multi-task learning approaches that combine segmentation with depth estimation or instance segmentation provide richer environmental understanding, crucial for tasks ranging from manipulation planning to obstacle avoidance. Figure 12: Meta\u0026rsquo;s Segment Anything semantic segmentation model 6D Pose Estimation enables precise robotic manipulation by providing the exact position ($x$, $y$, $z$) and orientation (roll, pitch, yaw) of objects in a scene. Modern approaches include: direct regression methods like PoseNet to keypoint-based approaches using PnP, while neural rendering techniques have emerged to handle challenging cases like symmetric and texture-less objects. Recent innovations in self-supervised learning and category-level pose estimation enable generalisation to novel objects29, while uncertainty estimation in pose predictions has become increasingly important for robust manipulation planning. Multi-view fusion techniques improve accuracy in complex scenarios, directly translating to more reliable and precise robotic manipulation capabilities in unstructured environments. Figure 13: Deep Object Pose Estimation for Semantic Robotic Grasping of Household Objects NVIDIA State Estimation State estimation acts as a bridge between perception and control in robotics, enabling systems to maintain an accurate understanding of both their internal configuration and relationship to the environment. While classical approaches relied primarily on filtering techniques, modern methods increasingly combine traditional probabilistic frameworks with learned components to handle complex, high-dimensional state spaces and uncertainty quantification. This integration has proven particularly powerful for handling the non-linear dynamics and measurement noise inherent in robotic systems.\nSensor fusion in robotics integrates data from multiple sensors, including joint encoders, inertial measurement units (IMUs), and force-torque sensors, to accurately determine a robot\u0026rsquo;s internal configuration. Traditional approaches relied on simple Kalman filtering, modern robotics demands more sophisticated techniques to handle inherently non-linear system dynamics. Extended Kalman Filters (EKF) and Unscented Kalman Filters30 (UKF) address this challenge by performing recursive state estimation through linearization around current estimates. For applications requiring more robust handling of multi-modal distributions, particle filters offer an alternative solution, though at higher computational cost. Accurate sensor fusion is particularly critical for complex rigid robots, where precise joint state estimation directly impacts both control performance and operational safety.\nFigure 14: Comparison of Gaussian Transformations, from left to right. Actual Sampling captures the true mean and covariance, EKF approximates them with linearization, while the Unscented Transform (UT) uses sigma points for a more accurate nonlinear transformation. Visual Inertial Odometry (VIO) enables mobile robots to estimate their motion by fusing visual and inertial data without relying on external reference points. Modern approaches like VINS-Fusion and ORB-SLAM3 achieve robust performance by tightly coupling feature-based visual tracking with inertial measurements. Deep learning has enhanced traditional VIO pipelines through learned feature detection, outlier rejection, and uncertainty estimation. End-to-end learned systems like DeepVIO31 demonstrate the potential of pure learning-based approaches, hybrid architectures have emerged as particularly effective, combining the reliability of geometric methods with the adaptability of learned components. These integrated systems are relatively mature and operate reliably in real-time while handling challenging real-world conditions including rapid movements32, variable lighting32, and dynamic obstacles33.\nYour browser does not support the video tag. Figure 15: VINS-Fusion, visual-inertial state estimation for autonomous applications.\nFactor graph optimisation provides a framework for sensor fusion and long-term state estimation in robotics. This approach represents both measurements and state variables as nodes in a graph structure, enabling efficient optimization over historical states to maintain consistency and incorporate loop closure constraints. Modern implementations like GTSAM and g2o have made these techniques practical for large-scale problems, while recent research has extended the framework to incorporate learned measurement factors. The field continues to advance through developments in robust optimisation34 for outlier handling, computationally efficient marginalisation schemes, and adaptive uncertainty estimation35. These theoretical advances have demonstrated practical impact in several robotic applications, including Simultaneous Localization And Mapping36 (SLAM) and object tracking.\nFigure 16: GTSAM Structure from Motion Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References P. F. Hokayem and M. W. Spong,Â Bilateral Teleoperation: An Historical Survey. Cambridge, UK: Cambridge University Press, 2006.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. J. Reinkensmeyer and J. L. Patton, \u0026ldquo;Can Robots Help the Learning of Skilled Actions?,\u0026rdquo;Â Progress in Brain Research, 2009.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Grauman, A. Westbury, E. Byrne, et al., â€œEgo4D: Around the World in 3,000 Hours of Egocentric Video,â€ IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos, M. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, â€œEPIC-KITCHENS-100: Dataset and Challenges for Egocentric Perception,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. A. Pomerleau, â€œALVINN: An Autonomous Land Vehicle in a Neural Network,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 1, 1989.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Ho and S. Ermon, â€œGenerative Adversarial Imitation Learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), vol. 29, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross, G. Gordon, and D. Bagnell, â€œA Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning,â€ in Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. Menda, M. Elfar, M. Cubuktepe, M. J. Kochenderfer, and M. Pavone, â€œThriftyDAgger: Budget-Aware Novelty and Risk Gating for Interactive Imitation Learning,â€ in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Zhang and K. Cho, \u0026ldquo;Query-Efficient Imitation Learning for End-to-End Autonomous Driving,\u0026rdquo; in Advancement of Artificial Intelligence (AAAI), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Ross and D. Bagnell, â€œReinforcement and Imitation Learning via Interactive No-Regret Learning,â€ arXiv preprint arXiv:1406.5979, 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al., â€œHuman-level control through deep reinforcement learning,â€ in Nature, vol. 518, no. 7540, pp. 529â€“533, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, â€œHigh-Dimensional Continuous Control Using Generalized Advantage Estimation,â€ in International Conference on Learning Representations (ICLR), 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust Region Policy Optimization,â€ in International Conference on Machine Learning (ICML), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, â€œProximal Policy Optimization Algorithms,â€ arXiv preprint arXiv:1707.06347, 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,â€ in International Conference on Machine Learning (ICML), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nH. van Hasselt, â€œDouble Q-learning,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nD. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€ in International Conference on Learning Representations (ICLR), 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. M. Smith, I. Kostrikov, and S. Levine, â€œDemonstrating A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning,â€ in Proceedings of Robotics: Science and Systems (RSS), 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nG. Williams, A. Aldrich, and E. Theodorou, â€œModel predictive path integral control: Information theoretic model predictive control,â€ in IEEE International Conference on Robotics and Automation (ICRA), 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nK. Chua, R. Calandra, R. McAllister, and S. Levine, â€œDeep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSutton, R. S. â€œDyna, an Integrated Architecture for Learning, Planning, and Reacting.â€ 1991.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nM. Janner, J. Fu, M. Zhang, and S. Levine, â€œWhen to Trust Your Model: Model-Based Policy Optimization,â€ in Advances in Neural Information Processing Systems (NeurIPS), 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, â€œEnd-to-End Object Detection with Transformers,â€ arXiv preprint arXiv:2005.12872, 2020.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Qiao, Y. Zhao, Z. Li, X. Qiu, J. Wu, and C. Zhang, â€œDeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection,â€ arXiv preprint arXiv:2108.09017, 2021.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, â€œEncoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation,â€ in European Conference on Computer Vision (ECCV), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZ. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: A Nested U-Net Architecture for Medical Image Segmentation,â€ in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support (DLMIA), 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. Poudel, S. Liwicki, and R. Cipolla, â€œFast-SCNN: Fast Semantic Segmentation Network,â€ in 2019 IEEE International Conference on Computer Vision (ICCV) Workshops, 2019,\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Chen, and P. DollÃ¡r, â€œSegment Anything,â€ arXiv preprint arXiv:2304.02643, 2023.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Wen, W. Yang, J. Kautz, and S. Birchfield, â€œFoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. A. Wan and R. van der Merwe, â€œThe Unscented Kalman Filter for Nonlinear Estimation,â€ in Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (AS-SPCC), Lake Louise, Alberta, Canada, 2000.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nL. Han, Y. Lin, G. Du, and S. Lian, â€œDeepVIO: Self-supervised Deep Learning of Monocular Visual Inertial Odometry using 3D Geometric Constraints,â€ in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Macau, China, 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Qin, P. Li, and S. Shen, â€œVINS-Mono: A robust and versatile monocular visual-inertial state estimator,â€ IEEE Transactions on Robotics, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nB. Bescos, J. M. FÃ¡cil, J. Civera, and J. Neira, â€œDynaSLAM: Tracking, Mapping and Inpainting in Dynamic Scenes,â€ IEEE Robotics and Automation Letters, 2018.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nP. Agarwal, G. D. Tipaldi, L. Spinello, C. Stachniss, and W. Burgard, â€œRobust Map Optimization Using Dynamic Covariance Scaling,â€ in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nT. Naseer, M. Ruhnke, C. Stachniss, L. Spinello, and W. Burgard, â€œRobust Visual SLAM Across Seasons,â€ in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nC. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, J. Neira, I. Reid, and J. J. Leonard, â€œPast, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age,â€ IEEE Transactions on Robotics, 2016.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/key-learning-paradigms-in-robotics/","summary":"\u003cp\u003eIn this post, we\u0026rsquo;ll explore the fundamental methods used to teach robots new skills. The three main paradigms we\u0026rsquo;ll explore are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eImitation Learning\u003c/strong\u003e: Teaching robots by showing them what to do\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReinforcement Learning\u003c/strong\u003e: Letting robots discover solutions through experience\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSupervised Learning\u003c/strong\u003e: Using labeled data to build core perception and planning capabilities\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these approaches tackles the fundamental challenges of robotic learning in different ways, and modern systems often combine them to leverage their complementary strengths. As part of this post, I have included open-source scripts for a robotic arm that solves a \u003ca href=\"https://robotics.farama.org/envs/fetch/pick_and_place/\"\u003epick-and-place\u003c/a\u003e task (similar to our coffee cup examples) using each of the methods discussed.  These scripts are available on GitHub at \u003ca href=\"https://github.com/AOS55/RLFoundations\"\u003eRLFoundations\u003c/a\u003e. Due to the natural challenges and computational expense of \u003ca href=\"https://www.natolambert.com/writing/debugging-mbrl\"\u003erobotic\u003c/a\u003e \u003ca href=\"https://andyljones.com/posts/rl-debugging.html\"\u003elearning\u003c/a\u003e, this repository also includes pre-trained models that can be downloaded from \u003ca href=\"https://huggingface.co/collections/AOS55/rlfoundations-67b325988a1b0f0b48d5cb68\"\u003eHugging Face\u003c/a\u003e. Please feel free to modify and use them as you see fit, they primarily demonstrate how to implement the IL and model-free RL methods discussed in this post on the simulated robot.\u003c/p\u003e","title":"Robotic Learning Part 2: Key Learning Paradigms in Robotics"},{"content":"To understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\nThis seemingly simple task illustrates why robot learning isn\u0026rsquo;t just about making predictions, it\u0026rsquo;s about making decisions that have physical consequences.\nSequential Decision Making Under Uncertainty $$ \\tau = (s_{0}â€‹,a_{0}â€‹,s_{1}â€‹,a_{1}â€‹,...,s_{T}â€‹) $$ where $s_{t}$ represents the state at time $t$ (like the position of the gripper and cup) and $a_{t}$ represents the action taken (like moving the gripper). Each action doesn\u0026rsquo;t just affect the immediate next state action, it can influence the entire future trajectory of the task.\nThis sequential decision making process is made even more challenging by the fact that robots must deal with uncertainty. These can be generally classified into 3 different types of uncertainty:\nPerception Uncertainty: When a robot observes the world through its sensors, what it sees is incomplete and noisy. Mathematically this can be written as $o_{t} = s_{t} + \\epsilon$ where $s_{t}$ is what the robot should ideally observe, and $\\epsilon$ represents noise. Real robots generally combine multiple sensors, each with their own challenges. Examples include:\nCameras, provide dense visual information. Computer vision deriving meaningful from digital images is an entire field in itself. In robotics we are usually concerned with any problem that causes the meaning of the image to be distorted, this could be visual occlusions, changes in lighting or changes to the key visual characteristics of the scene. Depth Sensors, measure the distance between to surfaces in a scene. They suffer from similar errors as cameras but are especially susceptible to errors from reflective surfaces and often struggle to detect small objects. Force Sensors, measure contact forces. These generally suffer from errors in calibration, either from misalignment or incorrect zero-ing of the force sensor. Joint Sensors, measure joint angle or position. Similar to force sensors they are susceptible to errors in calibration and alignment. Putting it all together Boston Dynamic\u0026rsquo;s Humanoid Atlas Robot has 40-50 sensors, as you can imagine this means there is a lot of uncertainty they need to deal with in order to understand the state of the robot. Your browser does not support the video tag. Action Uncertainty: Even when a robot knows how to behave, executing that action perfectly is impossible. For example in the simple coffee cup picking task there is still noise from mechanic imperfections, changes in motor temperature, latency in the control system, robotic wear and tear over time.\nEnvironment Uncertainty: The real world is messy and unpredictable. Physical properties can significantly vary the the way the robot needs to behave in our example:\nThe material the cup is made from could deform or be slippery The cup could have a different mass than expected The cup may not be where we expected it to be on the table Putting this all together, our robotic cup picking up algorithm needs to handle the following functions, each with its own sources of accumulating uncertainty:\ndef pick_up_cup(): cup_position = get_cup_position() # Perception planned_path = plan_motion(cup_position) # Planning actual_motion = execute_path(planned_path) # Control contact_result = grip_cup() # Sensing return contact_result This is why robotic learning algorithms need expertise that regular ML algorithms don\u0026rsquo;t:\nThey must be robust to noise The need to handle partial and imperfect information They must adapt to changing conditions They need to be cautious when uncertainty is high Linking Perception to Action At its core robot learning requires 3 key components:\nA way to perceive the world A way to decide what to do A way to execute that action With this in mind we can build a general model to account for each of these components. State Space A robot\u0026rsquo;s state space represents everything we can observe in the environment for the coffee picking robot this might include:\nstate = { \u0026#39;joint_positions\u0026#39;: [1.2, -0.5, 1.8], # Where are my joints? \u0026#39;joint_velocities\u0026#39;: [0.115, 0.00, -0.211], # How fast are they moving? \u0026#39;camera_image\u0026#39;: np.array([...]), # What do I see? \u0026#39;force_reading\u0026#39;: [200.1, 310.2, 0.9], # What do I feel? \u0026#39;gripper_state\u0026#39;: \u0026#34;OPEN\u0026#34; # What\u0026#39;s the state of my hand? } These states are constantly evolving and encompass a variety of dissimilar data-types.\nAction Space A robot\u0026rsquo;s action space defines what it can actually do in the environment this might include:\naction = { \u0026#39;joint_velocities\u0026#39; = [-0.13, 0.21, 0.55] # How fast to move each joint \u0026#39;gripper_command\u0026#39; = \u0026#34;CLOSE\u0026#34; # How to move my hand } Control loop Now that we understand state and action spaces, let\u0026rsquo;s explore how robots use this information to actually make decisions. The key concept here is the control loop - the continuous cycle of perception and control that allows robots to interact with the world.\ngraph LR A[Observe] --\u003e B[Decide] B --\u003e C[Act] C --\u003e A style A fill:#e1f5fe,stroke:#01579b style B fill:#fff3e0,stroke:#e65100 style C fill:#e8f5e9,stroke:#1b5e20 This control loop becomes far more interesting when we consider how to make decisions under uncertainty. This is where the concept of Markov Decision Processes (MDPs)1 become helpful. An MDP provides a mathematical framework for making sequential decisions when outcomes are uncertain. In the context of MDPs, at each time-step $t$:\nThe robot finds itself in a state $s_{t}$ It takes an action $a_{t}$, according to some policy $\\pi(s_{t})$ This leads to a new state $s_{t+1}$ with some probability $P(s_{t+1}|s_{t}, a_{t})$ The robot receives a reward $r(s_{t}, a_{t})$ The Markov part of the MDP comes from a key assumption:\nThe next state depends only on the current state and action, not on the history of how we got here.\nLet\u0026rsquo;s unpack what this means for our coffee cup picking robot.\nImagine our gripper is hovering $10cm$ above the cup. According to the Markov property to predict what happens when we move down $2cm$, we only need to know:\nCurrent state ($10 cm$ above the cup) Current action (move down $2cm$) Current sensor readings (force, vision, etc) It doesn\u0026rsquo;t matter how we got to this position, whether we just started the task, or if we have been trying for hours, or whether we previously dropped the cup. The trick is that the state needs to include all information that is important to make decisions. So if the number of times we dropped the cup is important to the decisions we make it should be included in our state.\nThis turns out to be very helpful. By carefully choosing what information to include in our state, we can capture all relevant history while keeping our problem definition simple and tractable.\nWhy this matters for Robotic Learning? The MDP framework is especially useful for Robotic learning for three key reasons:\nUncertainty: MDPs model probabilities explicitly. When grasping a cup, we can express that: \u0026ldquo;closing the gripper has an 80% chance of secure grasp, 15% chance of partial grip, and 5% chance of missing entirely.\u0026rdquo; Long-term consequences: Small errors compound over time. For example, a $1cm$ misalignment during grasping might let us pick up the cup, but could lead to spilling during transport. The MDP framework captures this through its reward structure and state transitions, even though each state transition only depends on the current state (Markov property), the cumulative rewards over the sequence of states let us optimize for successful task completion. A spilled cup means no reward, guiding the policy toward careful movements even if the cup is slightly misaligned. Algorithm design: The MDP framework helps shape how we think about robotic learning problems and building autonomous systems: Reinforcement Learning2 (RL) optimises for long-term rewards across state transitions. Model-Predictive Control3 (MPC) uses explicit models of state transitions to plan sequences of actions. Imitation Learning (IL)4 can learn from human demonstrations by modelling them as optimal MDP solutions. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References R. Bellman, Dynamic Programming. Princeton, NJ: Princeton University Press, 1957\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nR. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction, 2nd ed. Cambridge, MA: MIT Press, 2018\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nE. F. Camacho and C. Bordons, Model Predictive Control. London, UK: Springer, 2007.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nS. Schaal, Is imitation learning the route to humanoid robots?, Trends Cogn. Sci., vol. 3, no. 6, pp. 233â€“242, June 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/foundations-of-robotic-learning/","summary":"\u003cp\u003eTo understand why robot learning is fundamentally different from traditional machine learning, let\u0026rsquo;s start with a simple example. Imagine teaching a robot to pick up a coffee cup. While a computer vision system needs only to identify the cup in an image, a robot must answer a series of increasingly complex questions: Where exactly is the cup? How should I move to grasp it? How hard should I grip it? What if it\u0026rsquo;s fuller or emptier than expected?\u003c/p\u003e","title":"Robotic Learning Part 1: The Physical Reality of Robotic Learning"},{"content":"Robot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\nIntrodction In 1988, roboticist Hans Moravec made an observation: skills that humans find effortless, like mixing a drink, making breakfast or walking on uneven ground, are incredibly difficult for robots. Meanwhile, tasks we find mentally challenging, like playing chess or proving theorems, are relatively straightforward for machines. This counterintuitive reality, known as Moravec\u0026rsquo;s paradox, lies at the heart of why robot learning has become such an exciting and challenging field.\nThink about a toddler learning to manipulate objects. They can quickly figure out how to pick up toys of different shapes, adapt their grip when something is heavier than expected, and learn from their mistakes. These capabilities, represent some of our most sophisticated yet often least appreciated forms of intelligence. As Moravec noted:\nWe are all prodigious olympians in perceptual and motor areas, so good that we make the difficult look easy.1\nYour browser does not support the video tag. Figure 1: A robot placing balls in a pot.\nYour browser does not support the video tag. Figure 2: A baby placing balls in a box.\nThis is where robot learning emerges as a compelling solution. Traditional robotics relied on carefully programmed rules and actions - imagine writing specific instructions for every way a robot might need to grasp different objects. This approach breaks down in the real world, where even slight variations in lighting, object position, or surface texture can confuse these rigid systems. A robot programmed to pick up a specific coffee mug might fail entirely when presented with a slightly different one.\nRobot learning offers a fundamentally different approach. Instead of trying to anticipate and program for every possible scenario, we let robots discover solutions through experience and adaptation. Just as a child learns to grasp objects through trial and error, modern robots can learn from their successes and failures, gradually building up robust behaviours that work across diverse situations.\nPrerequisites To understand the approaches we\u0026rsquo;ll discuss, you should have:\nGood understanding of probability and linear algebra. Basic familiarity with machine learning and deep learning. Basic programming and computer science knowledge. Basic understanding of robotics/mechaniscs and control. What These Posts Cover We\u0026rsquo;ll explore how robot learning is tackling Moravec\u0026rsquo;s paradox:\nThe Fundamentals: Why simple robotic tasks are actually complex. Learning Paradigms: How to teach robots through demonstrations and experience. The Reality Gap: Why simulation alone isn\u0026rsquo;t enough, and what we can do about it. Modern Approaches: How new techniques are making headway on these problems. Real World Applications: How these techniques are being applied in the real-world. Citation Quessy, Alexander. (2025). Robotic Learning for Curious People. aos55.github.io/deltaq. https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/.\n@article{quessy2025roboticlearning, title = \u0026#34;Robotic Learning for Curious People\u0026#34;, author = \u0026#34;Quessy, Alexander\u0026#34;, journal = \u0026#34;aos55.github.io/deltaq\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;Feb\u0026#34;, url = \u0026#34;https://aos55.github.io/deltaq/posts/an-overview-of-robotic-learning/\u0026#34; } References Minsky, M. (1988). The Society of Mind. New York: Simon and Schuster.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"http://localhost:1313/deltaq/posts/an-overview-of-robotic-learning/","summary":"\u003cp\u003eRobot learning combines robotics and machine learning to create systems that learn from experience, rather than following fixed programs. As automation extends into streets, warehouses, and roads, we need robots that can generalise, taking skills learned in one situation and adapting them to the countless new scenarios they\u0026rsquo;ll encounter in the real world. This series explains the key ideas, challenges, and breakthroughs in robot learning, showing how researchers are teaching robots to master flexible, adaptable skills that work across the diverse and unpredictable situations of the real world.\u003c/p\u003e","title":"Robotic Learning for Curious People"},{"content":"Why is this blog called âˆ‡Q ? A couple of reasons:\nI started out in aerospace and max-Q (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter. My surname is Quessy. This blog is about answering Questions. How can I find out when a new blog comes out? I have an RSS feed that you can subscribe to. I also post on Twitter when a new blog comes out.\nHow can I get in touch? Email me alexander@quessy.io\n","permalink":"http://localhost:1313/deltaq/faq/","summary":"\u003ch3 id=\"why-is-this-blog-called-q-\"\u003eWhy is this blog called âˆ‡Q ?\u003c/h3\u003e\n\u003cp\u003eA couple of reasons:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eI started out in aerospace and \u003ca href=\"https://en.wikipedia.org/wiki/Max_q\"\u003emax-Q\u003c/a\u003e (âˆ‡Q=0) is the point where a spacecraft experiences the most force on departure and is key design parameter.\u003c/li\u003e\n\u003cli\u003eMy surname is \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euessy\u003c/em\u003e.\u003c/li\u003e\n\u003cli\u003eThis blog is about answering \u003cstrong\u003eQ\u003c/strong\u003e\u003cem\u003euestions\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-can-i-find-out-when-a-new-blog-comes-out\"\u003eHow can I find out when a new blog comes out?\u003c/h3\u003e\n\u003cp\u003eI have an \u003ca href=\"/index.xml\"\u003eRSS feed\u003c/a\u003e that you can subscribe to. I also post on \u003ca href=\"https://twitter.com/QuessyAlexander\"\u003eTwitter\u003c/a\u003e when a new blog comes out.\u003c/p\u003e","title":"FAQ"}]